{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaso129/2023dlAssignments/blob/main/A4_pretrainedModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6AaufM3kOlZ"
      },
      "source": [
        "# MIS 583 Assignment 4: Self-supervised and transfer learning on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuMvSPEkOlb"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 李晨愷 M114020035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpQrdUXNkOlb"
      },
      "source": [
        "**Your Answer:**    \n",
        "Hi I'm 劉捷生, M124020043."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWMWW8Ab_345"
      },
      "source": [
        "## Google Colab Setup\n",
        "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "\n",
        "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH4wc4iD_6w_",
        "outputId": "a2d534bb-b981-496b-fbcb-e4f55d3c7cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um5DJvBwb6xT"
      },
      "source": [
        "# Data Setup (5 points)\n",
        "\n",
        "The first thing to do is implement a dataset class to load rotated CIFAR10 images with matching labels. Since there is already a CIFAR10 dataset class implemented in `torchvision`, we will extend this class and modify the `__get_item__` method appropriately to load rotated images.\n",
        "\n",
        "Each rotation label should be an integer in the set {0, 1, 2, 3} which correspond to rotations of 0, 90, 180, or 270 degrees respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHkeNUOKiFbP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "def rotate_img(img, rot):\n",
        "    if rot == 0: # 0 degrees rotation\n",
        "        return img\n",
        "    #######################################################################\n",
        "    #        TODO: Implement rotate_img() - return the rotated img        #\n",
        "    #######################################################################\n",
        "    elif rot == 1:  # 90 degrees rotation\n",
        "        return transforms.functional.rotate(img, angle=90)\n",
        "    elif rot == 2:  # 180 degrees rotation\n",
        "        return transforms.functional.rotate(img, angle=180)\n",
        "    elif rot == 3:  # 270 degrees rotation\n",
        "        return transforms.functional.rotate(img, angle=270)\n",
        "    else:\n",
        "        raise ValueError('rotation should be 0, 90, 180, or 270 degrees')\n",
        "\n",
        "    #######################################################################\n",
        "    #                           End of your code                          #\n",
        "    #######################################################################\n",
        "\n",
        "class CIFAR10Rotation(torchvision.datasets.CIFAR10):\n",
        "\n",
        "    def __init__(self, root, train, download, transform) -> None:\n",
        "        super().__init__(root=root, train=train, download=download, transform=transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image, cls_label = super().__getitem__(index)\n",
        "\n",
        "        # randomly select image rotation\n",
        "        rotation_label = random.choice([0, 1, 2, 3])\n",
        "        image_rotated = rotate_img(image, rotation_label)\n",
        "\n",
        "        rotation_label = torch.tensor(rotation_label).long()\n",
        "        return image, image_rotated, rotation_label, torch.tensor(cls_label).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCBSpNWpb8uw",
        "outputId": "d6aec763-cbbf-4f04-a8a3-838fe1e5c3cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 37084957.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = CIFAR10Rotation(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR10Rotation(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCWMyGhVOJB"
      },
      "source": [
        "Show some example images and rotated images with labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "A9wN4BJWVMzB",
        "outputId": "463e413f-0dd7-417e-e05f-3cd0727edc55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACrCAYAAADGmf6bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFXklEQVR4nO29e5Ac5XX+f7p7rruzM7O70u5qJS0SSJa4GyQQC47j2EowdmE7UIntIrF8qfhrR3KM9avYxo6dihMiKqmKLymMKykHJxUTHL5lcIwvfInAEBwhQEY2IEsgEEhI2l3tZXb2Mpe+vL8/wNP9PCPtaEAaLdL5qFS1Z7un++23L/Nun+d9jmWMMaIoiqIoitIi7FPdAEVRFEVRzix08KEoiqIoSkvRwYeiKIqiKC1FBx+KoiiKorQUHXwoiqIoitJSdPChKIqiKEpL0cGHoiiKoigtRQcfiqIoiqK0FB18KIqiKIrSUnTwoSiKoihKSzlpg49bb71Vli1bJqlUStatWyePPfbYydqVoiiKoihvIKyTUdvle9/7nnzoQx+Sb33rW7Ju3Tr52te+JnfddZfs2bNHenp65vxsEARy6NAh6ejoEMuyTnTTFEVRFEU5CRhjZGpqSvr7+8W2G7zbMCeByy+/3GzcuLEW+75v+vv7zZYtWxp+9sCBA0ZE9L/+1//6X//rf/3/Bvx/4MCBht/1MTnBVKtV2bFjh9x0002139m2LevXr5dt27bVrV+pVKRSqdRio0V2z3hWr1oCsWXjNRFznDCwHVgWeFWIp2dm8bMxXD+WjEM85s5AfNXadRC/Z82VtZ/9sUO4rUQC4uePVCD+vz/6oSA+RIk2bMuMBBAXZ0vhJ2fxOMXHdR06rlx3nrZdxo/HPIhjCexzxwr7LU7ZWs/Dz0oSHyuOj/3y4vbDopw5rP8/fwWxMXitSt0zf47vAFrX0LqVCt4X5Qpe5zY9Lxx+fgTH3nc8jvdUIp3EOIFxLI73SUD3qO/i/e/74X0UBMExl4mIeB5+lp9rdW8dqM9nIs8SEZFSJH76v/5JXi8dHR0N1znhg4/R0VHxfV96e3vh9729vbJ79+669bds2SJ/9Vd/Vfd75czFcfDG4cEHLKebzDIY803I2+bYDjDVF0/gA6ctnar97KUStC4+fFJJanfda8g5jkvqBVm2HbYtsCklaaxjrnv0bWNsHPo8PRlsK1yfP2vTviVGyy3VtZ/JxJMpiPmL9UQOPny6Fp2A77HXPviI0eAjlsDjiiebG3zYNg4gLO/Ygw+LBh9i0x8LMbxhGw0+Yh79sUIDodfL8UgmTvjgo1luuukm2bx5cy0uFouydOnSU9gi5VRj8+DDohvRjv5MNxH9BZBO4wOiGrgQz9JN7CRxQPHEzl9AvLSjq/bz2gG8Tv0KvmVZtQyXd3ZkIR46MgKxRV/qSXpQdsQyYbvj+JdLxce/8C5csRrii1Ytg/ixXz8F8csFbItXxYewHelnI/jXpVuhB5uLD2k/Tl82ypkFDRhs+mIKaHn0i7daxWut6uL9y19y7W1tEPv0hV8q430SS+A91pYOP+/TAMDz6Qu6gm82Da0f92lAIHP/wWBFlrsVPM7p0hS2xcPlbek0xEn6Q6hUxufF1NQ0xGXql1ZwwgcfCxYsEMdxZHh4GH4/PDwsfX19desnk0lJ0ohRURRFUZTTlxP+PjSRSMiaNWtk69attd8FQSBbt26VwcHBE707RVEURVHeYJyUtMvmzZtlw4YNsnbtWrn88svla1/7mszMzMhHPvKRk7E7RVEURVHeQJyUwcf73/9+OXLkiHz5y1+WoaEhefOb3yw//elP60SoinI06qeHH1ssySJOi/QimRim9GZKmPsUUscn6JYokVr+vod/Vvu5/OY3w7JOyun29KGe5F1vuwriR/73EYiHxiYgTlqYC2+PiMS6aWaM34ZalVVL+iE+pxdTntMFnNUz+jLuO2AtTFvYLyUXc9sJwbbEXRLaNZrvr5zWsMaDRaJzCbFZIJqQY64qIiLxGF6LyRze/5kM6Y9IcBrVkLikqzCkH4mRsNohkTdrQDwWs9bpbsPlLBiPsQ7O8OyWuQXniTj2XJo0IjGn9fLPk7bHTZs2yaZNm07W5hVFURRFeYOif5IoiqIoitJSdPChKIqiKEpLOeU+H8di1Q/+Wpz2VzwaOI/vWOy+GP5s2HyJ84m0ODih5WNo3jYbrdTta25jmLp54UR0/bp9UX6RukwcWj9GscXmXGQUFXsd1g3/79IPN1gD59NzjthExsxs0utW8bNeQC6C5AwYkO9HVVDrUCLnz2LEMfXBp56EZWsHzsF9OeiAyvZFafIU6e7gPCzmdWdLoT4lm8QT0Eluix1l9AWYHRqC+CyqsdTdjXqsF156AeK4G2pf7CxeC53JToir1OfTNvohKGcWpVnUF9U95+g6j8Zsw1BvpkWaDcMeImzWhXG5hG1rxm3bIZ1EPE7OvmTcF5BR2GwJfYGiTsEWP/vpAc6Op8VJ0rLR89y2UAvDx8b91Ar0zYeiKIqiKC1FBx+KoiiKorQUHXwoiqIoitJS5q3mw3IcsV7N/bHmw7a4VkAkoHUDyrTXZbbqJCENdBtNYLH+pE7DwUXQjj3nXKRetwHLeV3uIz6u+ubi8jqtDM0zj/zcqA4xz/NvRH2/IX5EU1Clgkhcw6xKNRJmA9QfVGNc04D6jXLMSSuMj1A9lGep9EP5CJYYGBufhDhGBbdyVAkyToXrvNHw88UKajosF/1KushjxA3w2qpMFCA+/5yzIX75AGpEytNhftoiXUxyGW477mGfFUcxr96It310I8SpiCdBG9XuyGbaIe6g5e2JDC5P4nKLND97n30G4oceur/2c0AVjxd1d0NcnsFrKdOObevt64K4NDkO8Xmr3wTxy0Ph9TNyBNdN23RtkKfEC4dfhjhBz5pLyaNm8WL0hYk+m0p07ay57C0Qv6lvOcRcQO3/+9f/wLaS7or9NDz/2PoDLpCYoOJumTbscy4M6VMF5grVPJmeCu8r1kXE41TFNkUF80g3EZD2qTyL9VSqFdJpQL1MLq7Jz1/yFGFtC51v39Bzkp7JXGCvFeibD0VRFEVRWooOPhRFURRFaSk6+FAURVEUpaXMW81HlHoFwPFrCHiqNntG1Bl91Mk0jn9fdR81DT7Li1nT0WiHc81D52WNjoP7qW7bdR15/DSp+WBvDiOoZzBwzihX6WHDKoL5ZEmRv4mNGgDjcT0GzBF3RWrF9OdRo2FRnZhDowWIue4EZ1mrVdSjdGXR96N7cZjP3rWXfDwoTx5YuPULBq+AuDyN+Wd/926IezpRKzE6Fh5bzGCue2qGdDQW9pkkmrlYRMpjqJXp6A31CFYV+5B9G9LUx6k4+xvg+uNjoxDvfW4PxH6ktk8uS/oS0pskbNxXLIb9VBjHc5aK499+M2XUlJhIfZ3Fi9BLpY0e3XtfOgBxZ2ce25rCtvYsRP1J3Kb7JnItxm28Dtu4fgrFHvn0ZNqzENeVOKF7zIq0xWIRFz94SAOSIh1Vkmqa8M7b2/H+j9Y8cV18dsQTuC3WZfg+eQyRvsTpwH7I5rhfws9XqqgfqpTx2cIeIynSnzBl+jwfmx1XzYeiKIqiKKc5OvhQFEVRFKWlzNu0i7GtmlV6YPO0IJ4GGi7nLApnPhomAOrKP/MU1mN/tO7lcoN0Q52FLqU66to+x3HXtcXwcdByi6ckI3VTc+vs18O4LpVFNDtduSg4Bc0YfGVoRaaF2XQJu3F81WmSNH05wPUNpXgCOu6FNDXziv7QljwooD3yYZ4ySP3ieZiecOK4L7ZI7+nAtrcnwte0I534Cr9SxX1bFsbTZCPd17sA4swL2JauPL5q99wwDdPZlYNlUwaPa5rSR7EMHlcjRvc9C7ETeUUco/TBxEHso9SqlRB3n4WpsclxnLK6fdsjuO/hgxAvXrQwDCg94NH5ninhq3Kvgn2epPO9eOUSiHt7FkK8MHKtTRxGq/6+DkzDjBcmIK7M4L77+9E+f1EPThPe8yym3Ww7PGe2g8fZHsP0Q9LBtAtlXSSoS+FS6QZK20Q3wKmNOKXR4lSiwKbnmkf3BU+15bY50an1XIKCpt3zc43TLpUqpYtpXz59r3leuH6VPutTetHQ1NkqHSfD05/r2k7Tn1uBvvlQFEVRFKWl6OBDURRFUZSWooMPRVEURVFayrzVfMzNsXUZdenFpt3Rm5uSOreuY+5t1U9mJWtv0m28dqN3qW9ng2m+zcylbbRmcxMtRUwMG+cGnBMO85dBwPboVMa6QtM+66bqkf0+5WHPoum0fRHL7IKPY/dumtVnKAds+5jHtWmaYHcn26uj7sJEcuPnLFsKy8ZGj0DslVETsu2BH0Pc20OajzxqABYtQI1APKI3aeugqbZjL0LssuV9qbly3d5MEeLDL4QaEJumUk5P43FOHMK2JH8Lz//IMFriH3wBtQ7tabzWsulwSqof4Pkrl1Db4tJU6yU9aFm+6uwBiIeHsa27nkJr956I5XngcSl41EUt7Maps5Nkt5+gPzP3PvccxEdGxyBeser82s8l7GJpT6DuJsWaDdIX+KSViTl4oxiyhvci5eJZw8FaNp6Kyzo6tm5n7QQ/DxKR6bRBwDoLXDeZRC0TW/8naGou6zjicdaQmMi6eG2VqUwEl+JoS6fmXF4mG3m3gfalFeibD0VRFEVRWooOPhRFURRFaSk6+FAURVEUpaXMX82HJa9T5PDadxulWe+OZqjzJDmBu+J53HUlmvkDr18sc8LgfCQ3LRY5NkOdaGi6u036EXH4uMjng3LCCcqdmohPSKod/S56M9THNBe/PIPJ89yCHoiXDKDvQ6YdLc6rkZLcFmkXRscnIa642Gl93ZQTpra1U/n3VRecB3E+Yg0+dHC/IORfkiZPAZbdNCDu8N9E4Um1SE+SJM3P0Atoj37vMFq1V0gr4ZKOo3/h2RBn2sN+K5XpuEh/kFmMuovzzkHPkYtWYOn5PTHsmJ2/+iXET42EGpB8Dq+F58mvJN+Nmp0FuTzEL+5BbUsH2a/7Dl7nu/bsrf284qwLcV9Z3Jdj4deIY+Nx+T7F3twaoOiji63ay6SrKU6iPoi1C6yjY6+keAyPOx2xKbepT4IG9un8zGWNCPuExMkvRay5dBesD8Gltj338kYeJfzMbQX65kNRFEVRlJaigw9FURRFUVqKDj4URVEURWkp81fzEaVBvRaM567FUl8qnnZV/wEIg7oc4rG1EXVLGh2HNFjOG5xDltFIu1KXM6Q1Aso/tgnOaV8ehLUlpgzm3Q85WK6d6+M0gqbXi1Mhv4zIzxZaYYghgYFjYbttqu3iGlw/Liga6U6h94brh/0ySyXQLdKHSBWXL+5D74yOnkUQj0xgP05M43G7kf0dPISl4I9MYp2ZgErHpzN4HA7lm1/ah74PE7PYDyOR7c8WSbvioJ5kZBb1J+WguboRyXbcXiyiAfHJv8L2KJdNNS6KY6j58Eh/YJMGyKNS5plIyXWHcv7TVbzOsx14rU0V0Ttjdha9Vbo7USPSmcd6LWY63P4Y+XCMHcTjmihgW1ZccC7EJao7YwTPYZn0DRU/jC+/lDxh2lAfZFGZeoueJd3kQeKR5sOjuiLViKcFe2OYYO7nd5w8R9hLx7fYSwnj6P4c6hOO6/UjuG+H9k23ZL2PSOTYLAv3FY+znqSBpq/JujTsC9IK9M2HoiiKoigtRQcfiqIoiqK0lKYHHw8//LBce+210t/fL5ZlyT333APLjTHy5S9/WRYtWiTpdFrWr18vz5GVr6IoiqIoZy5Naz5mZmbk4osvlo9+9KNy3XXX1S3/u7/7O/nGN74h//qv/yrLly+XL33pS3L11VfLrl27JJVKHWWLx8CyQmEC5adYd2EiyxuOprjECaUQWVfhsxairpnhB6wGehJb2Gtj7vV5KfuCzOkDYubOwxruU0r5xWlnqwTn9g8GoSfF7gDrihx0UH/QtGHJNOZ5rTKdg0ie3tAJL1EdCe6HtjYUifBxL0+jNiJRxXz04ULor8EajxTVckgmMQc8S9qVxx5+AuJfvDgE8QzVpehoCzUFi7uwjkSa/EwcygGPjBUg5pxvhXLrRyZQtzEdaXuCctvtCexTu4LLK1SXohHpNvS0iCfCx5RPPh2TJdKA0HGn21CHUa3aFOPnJydJrxLZX74D22WoLUnSCxkflz//wl6Iu8iLo78PNUDZavj5Shl1ExMp1F3sehb9Tfa9+DzEFgkOli1eBnE5jufs0FjYD0v6FsOyJNVT8QzpMgSp107g1046TX4abWFb2UuDY66nwxo8jll/xl4eJvL8qJJ+KCC9CVsIGfLOMaR1ssgPha/VqEeJ57IOho8TP2uTto39TVyqFVOia5fv/1bQ9ODjmmuukWuuueaoy4wx8rWvfU3+4i/+Qt773veKiMi//du/SW9vr9xzzz3ygQ984PW1VlEURVGUNzwnVPOxb98+GRoakvXr19d+l8vlZN26dbJt27ajfqZSqUixWIT/iqIoiqKcvpzQwcfQ0CuvjXt7cUphb29vbRmzZcsWyeVytf9Lly496nqKoiiKopwenHKfj5tuukk2b95ci4vFoixdulQCMTWdAs/FZt1FdHmdFwYlIOsqe9AvXAu3zjYedRqRyFztRj4ddR4h0oCTWF6F61JwbYezfMxvn1dGLUQ8ks8upjA3Sml1ScwtbakjN0v+Fh6d/0iy1UYrDUmk8DiSgnGC2tabx+NancR4poi50WrE5yOXRd3FgoXoh5CIowakUsZ89WWXXgTxstWrIN63/yDEmXTYeEqTy5HCBMRJmtefIa1LMo054iQtj5GmYKYcnpNyCT0lLLojuxewNwoe9+Tzc2tAPPLTiEceUzbpBQx5JbhVqrdBN3CMamC4pNOYmUa90lQxPNbePOou+hahRmNoBP/Ayi3APjxv5QpsC/3tt4iun5cOHqj9XE1gu8/vxX1XpgoQe6QJWLwUdRv9ixZC/PIkntOe7rDu0NIly2CZIaEV+/LU6YkqqDdgrQNL5dj/ApbxyrSvaoW0DC7uu1qlZzB5ccQitWQC1o9ZqIthL47AwW1zGalkHO85h7RTc0EyG3E90oDQF1OM7hPWaZkE1amhfmoFJ/TNR19fn4iIDFMxp+Hh4doyJplMSjabhf+KoiiKopy+nNDBx/Lly6Wvr0+2bt1a+12xWJTt27fL4ODgidyVoiiKoihvUJpOu0xPT8veveGUsX379snOnTulq6tLBgYG5MYbb5S/+Zu/kZUrV9am2vb398v73ve+E9luRVEURVHeoDQ9+HjiiSfkd37nd2rxb/QaGzZskO985zvy2c9+VmZmZuTjH/+4FAoFectb3iI//elPm/P4kFd87mte95TPqtNWRDUfDXw7OmYxn5h38eXPRBI3MJuiOe00uZtzznO1k4/D4To0x9zSiYdbnfDwOM9zMW/fVcVLZdoOc4QHYjhDya4/8KbatqIH89ExH/ObbsT/oERz6y3KRzs+xj0xzMN3Uzw5gTljT3D7mbZQ55HPYS2OXDe2O0cagRzVxEhSvYa+Iuo2VvWj7qYwGfbzwWFctyufhxjVJiLZLO47m0G9CueEYz24/mgx1EIcGsJceIzq4yxI4rYLFtYRacTMDPnERK5Wj/wJyiXO6ZPXRiPvHcrbs7/CzGxYE2VyGgVG7UnM4WcyOYi5HssMzeRbQrV9qi7240BP6KUzU8RaPssW4rXW+a5rIX5i11MQX7H2Moj3HD4E8ejICMT9S88Lt51FLYrrck0SCIVL+fA58H32v+AaKOF9wfoP9vkIfFxeoToxs2V8dnjUdqtK9/tMeF8ZD5fFqc6T04H9YpJUkyiBcZx8PmKkV4n6obAuJpHA+zOgTuY+dsiDiLVO/H3c2ZWv/fy4tIamBx9ve9vb5ryhLcuSr3zlK/KVr3zldTVMURRFUZTTE63toiiKoihKS9HBh6IoiqIoLeWU+3wci8AytbnL7FNfP2I6ts8HJ4g4h79wBrc2EMM83vNlzPvtT2Le10TmicdoZz5pPLi+Ql2tF663QuktM9fn61Jh5P1PuguXhBkrfcxndhzEef9TpK2oLg1rvYxZNOec5+03qGHDtKcwl95GOeWyF+bhrTIunOWp+RnULhgH/SxGi6gZKE6VIc51YG40Wt8jH8mTiohY5K3hubityQLuy4mR3oC6qVTFY5uJ1FdJpfG4LPIUScTw/Hd2of6Ec8axGNepIU+DINRS2Db7GZDpCJkUkwSgIXwpVyL1W3yqE1MlXw/fn/ue4fvCIZ8H1iuMjRdqP2ezqOlI9eC1tHjxAMQl0vAMHcEaSMsWL4e4jbxXPDf8fCyD5y8ZR11Nexv2w1Kq3WJbXHcIr+uzBnD9c1eHHjTtKdwX6zDqfTr4HJBPD30+8PkKCc8J14Xh8+f65E9Bz9A20lklSEtRGsWLdbwQerUsXoDn+9yLzob4mRfRUiKI4fUg9Dzg43Tpmcp6FtzU3O8JXBfvC9bVOKQ3SZBvTDze+qGAvvlQFEVRFKWl6OBDURRFUZSWooMPRVEURVFayrzVfIiEeg3WfMyFTetyxnc6iTnBF9OYZ7sCpQ6ybhHmcWcr+yCecDCvP9fODYkhAvYvYQN/LiwTYGzPMXTkFCH3S5fB/PLFPnpWdHiTEMfbMGf8khPmWsuU1bcp72o18FpgyhPkj0C1QYLIXP5EErUJ/b15iP0E1RlxSMPRjsdlxTBPzxnnWDzM6xrKbXtV1L6USUCQovyzk0IfD4vytIbrlETqNRgL1811dkOczWO+ulQm/cnkOMT5duyXkoc3wlihEC4jbw2uC8ReG7OTzdWN8L2A4nB7Hu/bY78D3hppPOpuGvagwXgqUutleBSvjQXdqA/jfSXbMrQcG1eYxWNZsewsiMeK4fU0NobnK3Dx/CRIA5Tp7of4yCSef9vG+7+brpezB86p/cw1SjyPnnl1tV7m1ngwgeHzHZ5T1oukSA+WIh8ml+5Bj64Pr4qaPW8anzULOsJ7bMVZWCB1cTf28c9/vhficgJ1Ofk+/O5I0v0/XcK2ztVPrMlgrRLrRfi+sEjzEXcxTqfYGejko28+FEVRFEVpKTr4UBRFURSlpejgQ1EURVGUljJ/NR/G1BK4dfPIadWopqDOU4A+XKWc7zCm4WRnFWso/PYE5m1/K78Y4h+6z9V+no7jvuKUC2X/C56TzrlVbnv94nC5VZfLJo8ImnN+RRVrQ/SO4L6mKpiPbKd6KwftUBPCs9Prs4fNaT7qcsy0PBmpqbGQvBVWXLAC4ucPoLcC576XDKDXQnUW8/iHDu2HuBSp/eFNoC4mQfUS8l2oo6m42KfFI0MQV0mXwd4bUU+Z7ELMR2eymG8uzmA9lalZ3LaTwNx5hXQWFaqvEi15EtDfLGzTkMnhPZOxsV9GZA6dlIi47HcQ0QRUqVYHyQvqNR+sJ6DF7CNhyCjIdkJNkUt1REYjHiAiInYVtx7QlcvahtIMru8LnpOBpctqP3dk8VqaGUcNiEMeITm6HqYjmh0Rkf37dkFcdqmOiR0ed9zBZ4fvk6dEne8H9mE6jf4X7GfEeoXpiM6mSl45QlqnuIWfTZE2ItmOmq/JCdR8uDH8/NlLQq1MJov+JuOTqPnh+/sQPg4kIM+ZRAKfjDGqDRTtl3ovFfJ4Ij1grJ2+yOgcsJzEIx1dnb6wBeibD0VRFEVRWooOPhRFURRFaSnzNu1iXv0ncrRXqRgGVvhOyaLXTZT5kESpzqsbwhe78DXsqv2HIX5TgNPh3mSFr/X2JvBVdTlJ0wCpLb5FNsN0XPWTAPE3ycjU2wR91nFx3f5ZfPW9ys5DnM3iK8B0G70STOGr02k/kjJgX/lGHvcNCOLY1oyFr16tyMvzwjSmF7bvPoDtLOPOV5yNaZaFfX0QVydpSpuFr5i9SHpiisq7G04XUFlrn6dKOzhNuC2P6Qqf/jaoREqyjx4p4L7G8HVyLo+vhPuXvAm3XcF+Gx16AbdH1s8LIlMx42Tl7tLUWitO11IJz2cjvBKln6Kv6X2+1ujDbKfu8z1G09spxxdPkw15R3h/t3fgK3y+zIslPAclissVnFq7P8C02/4hLHN/ecTi/ILOZbCsN43ndzjAbRfKuO89+zF9eHgYrcFXLlsFcSaSKuFS8EI24l6AqTCfUiNuGdsWq7MdpzL3EcuBKqUqp2fwuIIK2qN7s7i8k+6DwMdn9Cw9P2wnnLJ+7nmrYZmTxunI01SKY3GVpr9S2YGyy1NrIYQ0K6cDy5QG91x81sSpT9k+n6csz1A/cpmCVqBvPhRFURRFaSk6+FAURVEUpaXo4ENRFEVRlJYybzUfbuCKHfwm70Xl4SkH6dph8izGiVjKq8V341Razo3FU1n8QAXzz4X9L0H8tmXh1M4lwzjXqiioCUi0YY6/kMblY3HMjU452PhqgHm5biucCrZsCqeF9Xo07dPHPHw5hrnPag9+XhyMCz72w3QikuenfKJdp/FobhpXQIl8chKWWERLEdDkyRJNMb30kkGIL7r4EoinC3g9uNTWhIXnbPTgwdrPU1VctyOJufAjM5TLjuG2KqSVmJxAvcrEGFo/+154fXQt6IFlZ52NOfuzz0GNR89C1LZUqnjtuTT1bmYM9Qe5jvB64OOYnML8se/hti281BrjzzE9tm5qbYO5tg30YhZNZ0604TnMZEIdzoIunL6aoOmJ5RmqzUDXkk8799hufQqfH7v3hNNhvSROrc3Q1NtDNvb53lHUk4xSyYL+RXgsPX2LIM7lIs9BnsZvsM8shy0F8PylaQq6TWUk4nFcPxYPt9+WxGf9xBg+h148gPfM5NBBiA8n8FqNpbDtTgXP2Z7ItOLuLtRBpfNoNzBFGg+L9EM2PZtSdCwsGopOtbWpz22ahm1oqjyvXwfpCzNteE781rur65sPRVEURVFaiw4+FEVRFEVpKTr4UBRFURSlpcxbzYf43iv/RaRMudVUgM3unAmXdwpqFTqOoI4i+RTOl7b60JY2RdOdl6RwLrdPHgSJSF5/mYV5WDdAXwCnjGO9WJVKkZNAxSMbao9y6Xbk9CUM5gB9yn27pNkIYpiPfOllnPc/TRbKYwncdyyS/rS68XyUad8J9mZoRAVFAmSHIRLxkUiQnXI2jX2cKGGufO+uX0I8S5ogl2zGD7/4IsQH9od54BLN2+ep8hXyNwg8tqXGc5Ak//0Y5WmzEdvyS9deDsuuestbIV7YgxoPmyyyDx3C3HgqTR4jVA4+mod3bDw/Nol8UqTJ6iZfiEb45JcSPaN1JQqavLTI2Vvicby4kuSXkIjk0jva8NlSLbElOX62yj4+db4OpEegz89EysM/P4t+Q/FyAeIx8vmo2LQzsnqfnERtVCaLHhbpjlDzYQlZ0NN165CfjUX7nqF+qrNXD+jZ5IYaIn8aNRmj+9CvZOogavACKlFQmqZzgE2VZAzFDvteDJ+DR0YehGUXrL0MP9y5BEKLrp14HHcWkJETe2t4kedDlb1zSCfD1w7bsbv0bPI8jB16HiQSzXnxnAj0zYeiKIqiKC1FBx+KoiiKorQUHXwoiqIoitJS5q3mw7i2mFfnUZ9dxnzU0jJqKZZKmJ9eluyGZYlu0nS8B3Pl7WnM4yZS5MVAubaxIdRGjAyFJdtjadxXwLlNmtc97WHeNclztX1c3zGYn/Rj4fJZypsmqd1cFyZGtQNcmpvfRjnAnhj6n5w9GmopfizYJ9UczUFvMi9vUyK/7JljLrdI41GZxrLXu36xDeLYQszTelSifXRkBOOJAsRVL8x3u5RWr/g0N5/ysgnybWE/iyqdMyeG56wSqccwNY3tWtSH2qQFpPmYmsW8ezKN53dhD/o82D7ljKth7j1OpcBt2rZFf9Pk2vLSDOwKA5cm6Q3q1mUfD7oHDcVp8vXIZfE6j57BON0zuQXo+1CaQS3M0Ah6bZQpj8/3u0s+PrOxcPlkGbUPbhm1TIkOfI6xTmNhDz4X2Supuwt9Y5IRH4nApfuPNHj8nONzMlPCfnFJ01MhjVdxOPTucOh+HiB/m2WXXArx7mefg7hM13H/4sUQZ9tR2xT1JImRb0dHNg/xhIvX/cgoHodhkQ9p37yANUGs0wmxSVfDXike+fTwtljrxL4gsXizZjyvH33zoSiKoihKS9HBh6IoiqIoLaWpwceWLVvksssuk46ODunp6ZH3ve99smfPHlinXC7Lxo0bpbu7WzKZjFx//fUyTOWbFUVRFEU5c2lK8/HQQw/Jxo0b5bLLLhPP8+QLX/iC/N7v/Z7s2rVL2ttf0Tt85jOfkR/96Edy1113SS6Xk02bNsl1110nP//5z5tq2Jpil8S9V/Kxq13MV+aF6rHEQy2Ew3UHDOaIyzTf2Z0mT4kS6ipKlKd9+QD6I0xPhLnYVIZqO5BBRTyB2w4Et+0l8HQ4MTzOWAxz7VEbASeOSb0seRK0ZVAnE60TIiJSLRchnp7Beh3VNH6+Mhsu71qE+xpif4MmzRgshzxLqEZCVAtRodosHnml2O3ovRIPsM8rk6jxmJnE+hqcz05F/S5IixLUlbDBtiQpb5uM49g/oDx9u4XH3d0RXh+Twy/Dspf2PQuxQ9deyWWtBGkfyOejrQP7LXDDc2KRH8VsCft0agqvpSzpERrBfxFFe7kuL37sNPnRoUuRc9+ei/dFKbJ8eAg1HAsXov7AsvD8lT3s86qPsU3nJJ7Ec+ZH6s649GwoTKNerK2Ex5FpQ/0ZawZWLsdaQAP9A7h+tGnkT8FeK/W6Gzwp42OoT2F/m4D6KarDeOu6C2DZukvOh3j5MjyOX+3CP4ZHxrGmzRVXYK0n46JWbmw8bGsmi/fExBT2+Y8e/F+Ih8ZRd2GoLpTd6GKNCJY86hOpewaSBqRBbRf2FPHJc4jr0rSCpgYfP/3pTyH+zne+Iz09PbJjxw5561vfKpOTk/Ltb39b7rjjDnn7298uIiK33367nHvuufLoo4/KFVdcceJariiKoijKG5LXpfmYfPWvxK5Xqz3u2LFDXNeV9evX19ZZvXq1DAwMyLZt2466jUqlIsViEf4riqIoinL68poHH0EQyI033ihXXXWVXHDBK6/GhoaGJJFISD6fh3V7e3tliF5Z/oYtW7ZILper/V+6dOlrbZKiKIqiKG8AXrPPx8aNG+Xpp5+WRx555HU14KabbpLNmzfX4mKxKEuXLpW3ltslbb2i+ZiawrneJdZxJML8l+1QbQ8aXwUu5roCmnM+OYO6jUNTVNPERj2CRDz0Z2cwb9aeQQ+BIulHCkcwH2lo7v3CfpyT3p5HL4dYPNSExNuwXVMVbLfrYKI2ToYI40OoZXnml49BnCYNwIqL19R+XjmF3giuhXqRQgyPuyE+5t2dJGpfMpH59lYR88kenc9UgG2JFbBfZsoYt1M9hk7y5rAj19M01XJxyA9hmoxAklRHZEE31tOwqe1OCa/FFYtD7VN7HvVDIy+/AHF3F+qkUhk8fyny6kik8Frt7EY9g18NtVE++RWkUtiHvof3XNxG/UEjSDIgJqqVoH3XeSk0wGJvDbonx0kjkIvUPMnlsQ+PFArYFlI/GN4XaTwC8onoIM8JK+IT46RJy0Z1gyYK+MaYHpESc/B6idt4LebaUNMVj3w1+KRN4x6v8/lgHU0FdXaMTzo8iYU6jHNX98OigeXYzuLoXogHLzkHYjt5HsSs03Fd1PwdGQ/7qUjfBe2ko1nel4f4V8+hDstJ4fIY3f8OXQ8m0m02CWts8vxhjYdhgxuC6wYl0hj7fpPP6BPAaxp8bNq0Se699155+OGHZcmS0LSpr69PqtWqFAoFePsxPDwsfX19R9mSSDKZlCQ9CBVFURRFOX1pKu1ijJFNmzbJ3XffLQ888IAsX74clq9Zs0bi8bhs3bq19rs9e/bI/v37ZXBwkDenKIqiKMoZSFNvPjZu3Ch33HGH/OAHP5COjo6ajiOXy0k6nZZcLicf+9jHZPPmzdLV1SXZbFY+9alPyeDgoM50URRFURRFRJocfNx2220iIvK2t70Nfn/77bfLhz/8YRER+epXvyq2bcv1118vlUpFrr76avnmN7/ZdMPaYpakX81zJbKYlvFonng0/2VT/QXL5roDuK0YeeS3pzE36jqYE8x0YW2QamS+dGA4x4/bnp0qQLzvuV9D/MyTqLPIUS2BnkW477bOsLZEz2Jc1pnHuhPtnagXyaTIQySFudRl56yA2BbMMVemQ3+NhQns40sr2IeTuCvZKnND0geJx/F8u9NhftunnH0qiZd0nIwdPMov5xK4vI28WOws5vklkjvvIK8FKeC1knTxQPLkG9DTnYfYL6E+xY+RfqU91NasWHkuLOvI4LZNlepMVOkeohSvW0X9gaF7TCL3iRUnHxbyM5AUtiWVxD5tBPtERGOfU9tkT8AeFHWZcFq/TNqJdBbz+rHI9TBTwj6amsGYvRl8qrcxSxoCi4xh2tvxHsxF6ivF6LrsyndBXCQfD/YUmSLfHq6fFCcNQToRnlOvrnYL6W4ECQLSNi3EZw8XGrHJeycohpMTDr2MBpXtKTyunf+7E+Lly94E8borL4PYb8fjHB7B7U/PhudocoruR8NaJjxyt4rXkmvh+U+Qds2K07OpFP0uwT4xAT7XYlQ3KiDNh6Hzb0ipw7otrjPTCpoafDQStYiIpFIpufXWW+XWW299zY1SFEVRFOX0RWu7KIqiKIrSUnTwoSiKoihKS3nNPh8nG8+tiBd7ZWzEc/nZht6O1B4wNMHd54wkpbZ8SiXFaYUEef/HqH5DvD3sQs4fjxXxs1OjWDfESWJufOX5b4a4m7wWDpNR28jup2s/51KkLwkwH51MUQ7RzkOcoDz96gsvxbbSPPDRSL2GdsrZ5jFVKgPYlIZYVMcincE8vBPR8dgp1B9kLNR0mADzrm2k6bFjWHeE63GU6VqL1pmxyUsla3Nth7mvLa714nTgORDKGRcnwo7NZDGPnmhHzxBO4c5Mon/FdAXPmUt6hip5M8xE9AoTVP+moxM9RaxZqo/CIp4GGKolEr1HOXfN1FWoYBEIacCSpEexyWtnJqLrqJaxT2bpfq+U8X43dC3ZpHVwYqSNmkSvDjtSnydGtTwc8jNiDUCd1wZpeIrTuK8KaaGsiF4tzjWlSMsWc6mGlYX9kOvMQ8yP5Bgdix25Dw6gxZPMPo/tnk2ihcPTB1FXU9rxHMQrx+nzs6jbmioWaj+PTVCdJxvP19A4frYjhzqctlwvxKkUaZ8MeUxFmjZNTt+JOH62nZ6JcfIQ8Unz4fmsR6K6Q9W5vVhOBvrmQ1EURVGUlqKDD0VRFEVRWooOPhRFURRFaSnzVvNhGV+sV/UbFs9RppyhK2HuzC1hHpZrtwTBsT0EROr9DbwZzJXvexFzayYZ5t7KlPNta0M9wZLFWKfgkkveDLFP+eb9z+6C2PIxL7dq+aLaz4NXvgWWdVJtj4By3ZVZqv1C+emOFLZlsoD5TceEbXFiWBfEF8ofNp6hDXT1YK60TUhLEbkeunqxT+OCx2HVtQXPd3oR1oLwJ7DGzaF9WDOl5IReG9mF2MdJmotvjWHCOpXB6yHXjboNK+KdIiJSIu+NqBeHcVFY48QHIJ6hOjPpBObty+QpMj1FGgDSfBSnw1x6cQrz6gt60J9CLLwHZ8q4fkPYuyOilTAs6mBxC69A2ocY+QCxtmW2hPewE6mJwWUgfHpW8LZS5M3BVgVV8oWYIh0G6tHwsw4dRyyG1x7XrOHP731pH8T7hw9BvHjRWcfcl9BzKsGymgT6WRQmChBXSF/Aj4dsRM8Q0LNlAm8R8T3yr6Hn/fjz6ONxgHR3pakRiB0rvJ7aMlizqkpapCkP+6WL6sbYMfL1oO+xafKJmZ0JvXlKfB065KVj1ambcH26HhzSjLAGJE4eJK1A33woiqIoitJSdPChKIqiKEpL0cGHoiiKoigtZd5qPl4x5HglRxaQd0dAubdo6LuYT+SaFa6LudCKh7k138WaGKUy5ginXJqTHg/9FRaTpuPCiy+BeFEfahk8ysvu3Ytz0ovDL0O8ZAHm1jsW5ms/7352Dyy75OI1ECcSmDOcpfyyTfnIXAL1DFMB1cBIhvnOgLO2lGdvVvPRkUfPik6HNB8RXxHLI++UDNa0SSYxb+s7mPvM9q+EeGpmHGKH6pi0R/LAqSxum8UKi85aBnHhZcyzt7t4DuJ0nB6dM4kc99jQS7jnLF576RzqSaoB+cBUSK9Amp+Kj9dDKhNee7EKtvP5F/G4PPKEOVTFvHqzRP0xDF1brKNoVAHCJ02Awyvw551IP1CftKVQj9CWxNilZ1G5gveQQ94cNh1bLp+v/ZyIs/8IanYKBTLEIBzWvnBNK2q7E+lztkrhPrPIr8Z28AOjR1Co4VJhoYDi0nSoP3C7UEfRlsFng+3QV1gMj6tCkqDxIn4fBCU8R+mIh8mRUezTWQ+3XbZQw+XQs0Us1hOSXxUZViUj9bYsYa0SaTjofPK1w5oOl/dN61sNNCQnA33zoSiKoihKS9HBh6IoiqIoLUUHH4qiKIqitJR5q/kIAhPRdlB+i5OQEe+GOOVR64q5UG6r4qHGY7qEfhbFKZwnLmXy3M+EOo4kpnRlcj/OnZ54CX07ZqiWw8sHMXc+NYp5/eXnXwhxf//y2s/jU9juTBvuO0U1LEaPYJ51agY/v38ftqVKngaQY7S5fg7n4ZsTfWQD9IXgOe7x3vC44xbmcCcOHIB4lpL4naS7cYefhZi9F7KLlkHsRUxmvDJeO2nKRy9chnqSyuH92NZh9BTJtqOmJ6DselStMEXeCfFxvE5THXn8LGubqOaNR7fJFHkQjIyFuo2ZWdQbsG/DjGC/jPp0DzWAvXjmhHPVdc8GjC3eNOk4nLrqMOFyj3w8LPqsS3qSKuXd8ws6adv4XEu3Yb2O7s6wVkj/4kWwzKU+f2YXPlv4nDjUTwuoHk8H6VfikRpWhoyVDPWRR/e/R+cg20E+MAT7X0T1Bx6dPz7ujgxqsgxpWXwfz0mJ6lDFbXxORms7lVzyiKIaN2QZJA5pXWJUo8oir6VYHONERF8WuHjt8HUeIw0Qa5XY78RwbReucRY0Kcw7AeibD0VRFEVRWooOPhRFURRFaSnzNu1iiSPWq6+dLWvuqVxQZpsskJMGX2X6NKUwmaBy7VTmmqdPBTR3K/py68gBnCo7OYxpE36lXyUreJem+Tr0KmzoEE69rXjhq7hMF77yHx/FV92ZNL76TJHddoxnx9J7uTiXA49Mr+OsC2dZfH7H1wBH8NVolayGvUg6yrXwfHKfWbSt0tgRiMvUT1XBFE97z1KIK2PhOfCnMP1gqA8nX8brwQie7zLlOiwqa+9T+iGIrF8p47pjR4awnVT+e8XKc7FxVKJ9mvZdommhs7PhtTlLVuxBO14bUx72izvT3Pnnv4iiNuN1GbwmU3psx87XJpeihzQOPTtc6iND114sTfcY2ZQXi5hejNG07vFIai2Xx2ndbWlMk6xetQpitoLnaZ79S/C6PjyO02HT6TB9meugdBE/K+pm2tO07mm8HhibUuVO5NmUsOkrip5DLqcnHJpySvfY5DT2eZnS1RKxdbDpu8AnuwHfwzhOqatcF053j9E54XIcvhd2ZIws7NnKny0jfEqrlOj5UKb726L0FF8vrUDffCiKoiiK0lJ08KEoiqIoSkvRwYeiKIqiKC1l3mo+4vGkxF/NgXJ+qs7fNyI6qFuXYratbWtHrUR7Lg/xgi7MjZZmUJdRqYb5TM7R874CytsZin0fp5Hx9CdaXTw3zDkGZOVcLBRwW2XMEcZSmM/knB/rOHgGI9jxssW18LrNjXHbFi6GOHqcIiLVYmh7bGhv7TmyPPfxszgBTcSnnHLcyUDMGiA3YsfsVfBacMdIdzGNVu1pskv3aGplnKbPpZN4wssRPYoXw8+mqYT2oUNYIj2ZQs3P0qXLIJ6lqbVT0zSNuD3c31QV8+azPsZV0kJMHC5IU9CURJiJWXcPkcankU00T73lWeKUO49eunT7iaG5lqzp4BuhMF6AuExTd2N0rSUilueHDx2GZWwTn+vMQ7zinBUQsw5j936cSv8/jz4OcTYR3kcrzsEp46w36M53QbzusssgjlE5d5fKSvDzIhYRoCXodLLNAj8jyyXUl0zSc9At4bUqVbzOo8/kRAqfiW0ZfLYkYnhPVagtY2OoJ2nvoGuXy4ZEpjTbCSojIPxdwCVH8FpMkqbPZu0MdbpqPhRFURRFOe3RwYeiKIqiKC1FBx+KoiiKorSUeav5cBxHnFdzqA7lUg3N5Q6sqOYDk4R1OWD2pKBcaDqGOf90Assmd7Rj3s+Lzv3m0tPktcAreB7mfCtV8rOgvG7doUQ8SCwquRyP09z5GPt04Lbq+g0XzzlKrdN40KedJjUfX/y3h5pa/8xk96luwMmDSyT4oUbA8vBqszkX3sD3g63B6697Lg3ASg/cWhSPbMjtCm6b8+4O2Z+w78/4keHIMtQysFX3wYNo1T86WoC4RJ4SkwX0u5menITYr4bH/eDD+Pzl51AsjhblKx5EvUls8fkQ23HybaJncFR34ZG+hEvBC5UJsOmZ2pEkvwwHtVIi2PYg4vuSJA1WgnQUHj3XMmnScCVx21V6ngd0vUSf2fzs90gPyDo6kvxFSpO8um3ShFWpX7lkQivQNx+KoiiKorSUpgYft912m1x00UWSzWYlm83K4OCg/OQnP6ktL5fLsnHjRunu7pZMJiPXX3+9DA83V1RKURRFUZTTm6YGH0uWLJFbbrlFduzYIU888YS8/e1vl/e+973yzDPPiIjIZz7zGfnhD38od911lzz00ENy6NAhue66605KwxVFURRFeWPSlObj2muvhfjmm2+W2267TR599FFZsmSJfPvb35Y77rhD3v72t4uIyO233y7nnnuuPProo3LFFVc01TDf92t+9ZyXtSnfiZ4UdcUfIOJtcQ6R9SVOnMdnnGOO6jpYKUFaFa4rEWBuO+lRHZkGpcVNZPuWPXcelWP2P6nLlHOl8jlbMvdnpbnSHsoZzoLF6PPiu6FeYeIgeqn4ZdRJNLxw6f5nDUidoGHOC5/XnfueYh8gNhkxwrVjwrhIWob6duKj/OBBrCvlVqkmVYVy/D55b0RK0XvkKWHTs6ZaRY+YZ369E+LfevOVMid0LKVyeKwJ0q61taEGrzSDvh0W1eJKdKDugs/JMNVnikd0OUEZ+3xsEtct0zlJpdAzpLMTa+JYDh4L6zZMRK8UGH5ocszfW3j+PQ/PZzxOfkhtqF9ppJU6GbxmzYfv+3LnnXfKzMyMDA4Oyo4dO8R1XVm/fn1tndWrV8vAwIBs27btmNupVCpSLBbhv6IoiqIopy9NDz6eeuopyWQykkwm5ROf+ITcfffdct5558nQ0JAkEgnJ5/Owfm9vrwwNDR19YyKyZcsWyeVytf9Lly495rqKoiiKorzxaXrwsWrVKtm5c6ds375dPvnJT8qGDRtk165dr7kBN910k0xOTtb+HzhwoPGHFEVRFEV5w9K0z0cikZAVK16Zx71mzRp5/PHH5etf/7q8//3vl2q1KoVCAd5+DA8PS19f3zG3l0wmj+orb4yp5cC4loBNOchYZA4z+1vU60XsOZc3Wp899h0LFgJBgNvirJ2hsV8igfnKRm0zJvJ5M7cqgz8bsJ9B/SdwOQ1TTTM5wqYEI8qZzgUXvxniwthY7efJcUzLVqrohxC3SLNFGi4/4JoY7OuBd2nAWqkISbpfWRvhelQnhp4ANt2T7OUQ1YhRCl9i7BmSpBons/gB36d9u1TliOtSReO6BxdtiwpB2eRfsSCbhrhCtX+49kt3NvTL4Bo2rJOp8LOF+sUiQyOba3055L0ReSbPUO0mkoBIMtON26ZrpTiFH7AdrmmD/RjVtySTrP/DfZcrqLOJk56En7ncxyn+zm1UE+kk8Lp9PoIgkEqlImvWrJF4PC5bt26tLduzZ4/s379fBgcHX+9uFEVRFEU5TWjqzcdNN90k11xzjQwMDMjU1JTccccd8rOf/Uzuu+8+yeVy8rGPfUw2b94sXV1dks1m5VOf+pQMDg42PdNFURRFUZTTl6YGHyMjI/KhD31IDh8+LLlcTi666CK577775Hd/93dFROSrX/2q2LYt119/vVQqFbn66qvlm9/8ZlMN+s3r/FKpdMx1+PVW9NVqq9MuluVHFwJ1aZe6qbb0LrXBtOCTmnap+7g15/Jo2qVhCiZo/TQu5Y2LW8XX1VGLbb6H+Nrj69riNIrhNAunH+dejtuauy2N7ou6xQ22N9e+LeqXuoma/N6ep3LO1U/UDn5UcDstiquVMsWYduG22ZFpo/VpF9y7W63Mudzy+XmPscefjyQDfBeX+eRhzsulLkVH6Sj6PuC0ix05Bw6ll3yyked2Ww5/t2BLPAe35/KX1QlOuxxPWt4yTSXvTz4vv/yyznhRFEVRlDcoBw4ckCVLlsy5zrwbfARBIIcOHRJjjAwMDMiBAwckm802/qAiIiLFYlGWLl2q/dYE2mevDe235tE+e21ovzXPqegzY4xMTU1Jf39/vbElMe+q2tq2LUuWLKmZjf2mjozSHNpvzaN99trQfmse7bPXhvZb87S6z3K53HGtp1VtFUVRFEVpKTr4UBRFURSlpczbwUcymZS//Mu/PKoBmXJstN+aR/vstaH91jzaZ68N7bfmme99Nu8Ep4qiKIqinN7M2zcfiqIoiqKcnujgQ1EURVGUlqKDD0VRFEVRWooOPhRFURRFaSnzdvBx6623yrJlyySVSsm6devkscceO9VNmjds2bJFLrvsMuno6JCenh553/veJ3v27IF1yuWybNy4Ubq7uyWTycj1118vw8PDp6jF849bbrlFLMuSG2+8sfY77bOjc/DgQfmjP/oj6e7ulnQ6LRdeeKE88cQTteXGGPnyl78sixYtknQ6LevXr5fnnnvuFLb41OL7vnzpS1+S5cuXSzqdlnPOOUf++q//uq4e0pneZw8//LBce+210t/fL5ZlyT333APLj6ePxsfH5YYbbpBsNiv5fF4+9rGPyfT0dAuPovXM1W+u68rnPvc5ufDCC6W9vV36+/vlQx/6kBw6dAi2MS/6zcxD7rzzTpNIJMy//Mu/mGeeecb8yZ/8icnn82Z4ePhUN21ecPXVV5vbb7/dPP3002bnzp3mXe96lxkYGDDT09O1dT7xiU+YpUuXmq1bt5onnnjCXHHFFebKK688ha2ePzz22GNm2bJl5qKLLjKf/vSna7/XPqtnfHzcnHXWWebDH/6w2b59u3nhhRfMfffdZ/bu3Vtb55ZbbjG5XM7cc8895pe//KV5z3veY5YvX25KpdIpbPmp4+abbzbd3d3m3nvvNfv27TN33XWXyWQy5utf/3ptHe0zY3784x+bL37xi+b73/++ERFz9913w/Lj6aN3vvOd5uKLLzaPPvqo+Z//+R+zYsUK88EPfrDFR9Ja5uq3QqFg1q9fb773ve+Z3bt3m23btpnLL7/crFmzBrYxH/ptXg4+Lr/8crNx48Za7Pu+6e/vN1u2bDmFrZq/jIyMGBExDz30kDHmlQswHo+bu+66q7bOr3/9ayMiZtu2baeqmfOCqakps3LlSnP//feb3/7t364NPrTPjs7nPvc585a3vOWYy4MgMH19febv//7va78rFAommUya//iP/2hFE+cd7373u81HP/pR+N11111nbrjhBmOM9tnR4C/R4+mjXbt2GRExjz/+eG2dn/zkJ8ayLHPw4MGWtf1UcrRBG/PYY48ZETEvvfSSMWb+9Nu8S7tUq1XZsWOHrF+/vvY727Zl/fr1sm3btlPYsvnL5OSkiIh0dXWJiMiOHTvEdV3ow9WrV8vAwMAZ34cbN26Ud7/73dA3Itpnx+K//uu/ZO3atfIHf/AH0tPTI5dccon88z//c235vn37ZGhoCPotl8vJunXrzth+u/LKK2Xr1q3y7LPPiojIL3/5S3nkkUfkmmuuERHts+PhePpo27Ztks/nZe3atbV11q9fL7Zty/bt21ve5vnK5OSkWJYl+XxeROZPv827wnKjo6Pi+7709vbC73t7e2X37t2nqFXzlyAI5MYbb5SrrrpKLrjgAhERGRoakkQiUbvYfkNvb68MDQ2dglbOD+688075xS9+IY8//njdMu2zo/PCCy/IbbfdJps3b5YvfOEL8vjjj8uf/dmfSSKRkA0bNtT65mj365nab5///OelWCzK6tWrxXEc8X1fbr75ZrnhhhtERLTPjoPj6aOhoSHp6emB5bFYTLq6urQfX6VcLsvnPvc5+eAHP1grLjdf+m3eDT6U5ti4caM8/fTT8sgjj5zqpsxrDhw4IJ/+9Kfl/vvvl1Qqdaqb84YhCAJZu3at/O3f/q2IiFxyySXy9NNPy7e+9S3ZsGHDKW7d/OQ///M/5bvf/a7ccccdcv7558vOnTvlxhtvlP7+fu0zpWW4rit/+Id/KMYYue222051c+qYd2mXBQsWiOM4dbMMhoeHpa+v7xS1an6yadMmuffee+XBBx+UJUuW1H7f19cn1WpVCoUCrH8m9+GOHTtkZGRELr30UonFYhKLxeShhx6Sb3zjGxKLxaS3t1f77CgsWrRIzjvvPPjdueeeK/v37xcRqfWN3q8hf/7nfy6f//zn5QMf+IBceOGF8sd//Mfymc98RrZs2SIi2mfHw/H0UV9fn4yMjMByz/NkfHz8jO/H3ww8XnrpJbn//vtrbz1E5k+/zbvBRyKRkDVr1sjWrVtrvwuCQLZu3SqDg4OnsGXzB2OMbNq0Se6++2554IEHZPny5bB8zZo1Eo/HoQ/37Nkj+/fvP2P78B3veIc89dRTsnPnztr/tWvXyg033FD7WfusnquuuqpuGvezzz4rZ511loiILF++XPr6+qDfisWibN++/Yztt9nZWbFtfLQ6jiNBEIiI9tnxcDx9NDg4KIVCQXbs2FFb54EHHpAgCGTdunUtb/N84TcDj+eee07++7//W7q7u2H5vOm3lklbm+DOO+80yWTSfOc73zG7du0yH//4x00+nzdDQ0Onumnzgk9+8pMml8uZn/3sZ+bw4cO1/7Ozs7V1PvGJT5iBgQHzwAMPmCeeeMIMDg6awcHBU9jq+Ud0tosx2mdH47HHHjOxWMzcfPPN5rnnnjPf/e53TVtbm/n3f//32jq33HKLyefz5gc/+IH51a9+Zd773veecdNGo2zYsMEsXry4NtX2+9//vlmwYIH57Gc/W1tH++yVmWdPPvmkefLJJ42ImH/4h38wTz75ZG1WxvH00Tvf+U5zySWXmO3bt5tHHnnErFy58rSfajtXv1WrVfOe97zHLFmyxOzcuRO+HyqVSm0b86Hf5uXgwxhj/vEf/9EMDAyYRCJhLr/8cvPoo4+e6ibNG0TkqP9vv/322jqlUsn86Z/+qens7DRtbW3m93//983hw4dPXaPnITz40D47Oj/84Q/NBRdcYJLJpFm9erX5p3/6J1geBIH50pe+ZHp7e00ymTTveMc7zJ49e05Ra089xWLRfPrTnzYDAwMmlUqZs88+23zxi1+Eh7/2mTEPPvjgUZ9jGzZsMMYcXx+NjY2ZD37wgyaTyZhsNms+8pGPmKmpqVNwNK1jrn7bt2/fMb8fHnzwwdo25kO/WcZEbPcURVEURVFOMvNO86EoiqIoyumNDj4URVEURWkpOvhQFEVRFKWl6OBDURRFUZSWooMPRVEURVFaig4+FEVRFEVpKTr4UBRFURSlpejgQ1EURVGUlqKDD0VRFEVRWooOPhRFURRFaSk6+FAURVEUpaXo4ENRFEVRlJby/wMiA0oDpOoYSgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class labels:  plane deer  cat   bird \n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rotation labels:  270   270   0     270  \n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "rot_classes = ('0', '90', '180', '270')\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    # unnormalize\n",
        "    img = transforms.Normalize((0, 0, 0), (1/0.2023, 1/0.1994, 1/0.2010))(img)\n",
        "    img = transforms.Normalize((-0.4914, -0.4822, -0.4465), (1, 1, 1))(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, rot_images, rot_labels, labels = next(dataiter)\n",
        "\n",
        "# print images and rotated images\n",
        "img_grid = imshow(torchvision.utils.make_grid(images[:4], padding=0))\n",
        "print('Class labels: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
        "img_grid = imshow(torchvision.utils.make_grid(rot_images[:4], padding=0))\n",
        "print('Rotation labels: ', ' '.join(f'{rot_classes[rot_labels[j]]:5s}' for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unCucbHexG4W"
      },
      "source": [
        "# Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pptQRpqK0rOl"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def run_test(net, testloader, criterion, task):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    avg_test_loss = 0.0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for images, images_rotated, labels, cls_labels in testloader:\n",
        "            if task == 'rotation':\n",
        "              images, labels = images_rotated.to(device), labels.to(device)\n",
        "            elif task == 'classification':\n",
        "              images, labels = images.to(device), cls_labels.to(device)\n",
        "            #######################################################################\n",
        "            # TODO: Calculate outputs by running images through the network       #\n",
        "            # The class with the highest energy is what we choose as prediction   #\n",
        "            #######################################################################\n",
        "            outputs = net(images)\n",
        "            _,predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            #######################################################################\n",
        "            #                           End of your code                          #\n",
        "            #######################################################################\n",
        "            avg_test_loss += criterion(outputs, labels)  / len(testloader)\n",
        "    print('TESTING:')\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')\n",
        "    print(f'Average loss on the 10000 test images: {avg_test_loss:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf698c16A9k5"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs=30):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = init_lr * (0.1 ** (epoch // decay_epochs))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lYdnb1Wsta_"
      },
      "source": [
        "# Train a ResNet18 on the rotation task (9 points)\n",
        "\n",
        "In this section, we will train a ResNet18 model **from scratch** on the rotation task. The input is a rotated image and the model predicts the rotation label. See the Data Setup section for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "knAiwdURvBHk",
        "outputId": "115c9d6b-6160-43af-d601-c392733ca52e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me-J5-Z_kOlg"
      },
      "source": [
        "### Notice: You should not use pretrained weights from ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "235MEIUgsv65",
        "outputId": "a8083afd-cf07-4a1d-d501-e0d8911bd0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "net = resnet18(weights = None, num_classes=4) # Do not modify this line.\n",
        "net = net.to(device)\n",
        "print(net) # print your model and check the num_classes is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuhiw0ZoszAd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "################################################################################\n",
        "# TODO: Define loss and optmizer functions                                     #\n",
        "# Try any loss or optimizer function and learning rate to get better result    #\n",
        "# hint: torch.nn and torch.optim                                               #\n",
        "################################################################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "################################################################################\n",
        "#                               End of your code                               #\n",
        "################################################################################\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WleH-YBgs0rq"
      },
      "outputs": [],
      "source": [
        "# Both the self-supervised rotation task and supervised CIFAR10 classification are\n",
        "# trained with the CrossEntropyLoss, so we can use the training loop code.\n",
        "\n",
        "def train(net, criterion, optimizer, num_epochs, decay_epochs, init_lr, task):\n",
        "    net.to(device)\n",
        "    criterion.to(device)\n",
        "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "        running_total = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        net.train()\n",
        "\n",
        "        for i, (imgs, imgs_rotated, rotation_label, cls_label) in enumerate(trainloader, 0):\n",
        "            adjust_learning_rate(optimizer, epoch, init_lr, decay_epochs)\n",
        "            ######################################################################################################\n",
        "            # TODO: Set the data to the correct device; Different task will use different inputs and labels      #\n",
        "            # TODO: Zero the parameter gradients                                                                 #\n",
        "            # TODO: forward + backward + optimize                                                                #\n",
        "            # TODO: Get predicted results                                                                        #\n",
        "            ######################################################################################################\n",
        "            if task == 'rotation':\n",
        "                inputs, labels = imgs_rotated.to(device), rotation_label.to(device)\n",
        "            elif task == 'classification':\n",
        "                inputs, labels = imgs.to(device), cls_label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            predicted = torch.argmax(outputs, 1)\n",
        "            ######################################################################################################\n",
        "            #                               End of your code                                                     #\n",
        "            ######################################################################################################\n",
        "\n",
        "\n",
        "            # print statistics\n",
        "            print_freq = 100\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # calc acc\n",
        "            running_total += labels.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if i % print_freq == (print_freq - 1):    # print every 2000 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / print_freq:.3f} acc: {100*running_correct / running_total:.2f} time: {time.time() - start_time:.2f}')\n",
        "                running_loss, running_correct, running_total = 0.0, 0.0, 0.0\n",
        "                start_time = time.time()\n",
        "        ######################################################################################################\n",
        "        # TODO: Run the run_test() function after each epoch; Set the model to the evaluation mode.          #\n",
        "        ######################################################################################################\n",
        "        net.eval()  # Set the model to evaluation mode\n",
        "        run_test(net, testloader, criterion, task)\n",
        "        net.train()  # Set the model back to training mode\n",
        "        ######################################################################################################\n",
        "        #                               End of your code                                                     #\n",
        "        ######################################################################################################\n",
        "\n",
        "    print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u4AsfAKtaQS",
        "outputId": "733d3e9b-07e1-4c97-ec75-209b0ba84e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 1.462 acc: 35.18 time: 16.35\n",
            "[1,   200] loss: 1.229 acc: 44.02 time: 11.42\n",
            "[1,   300] loss: 1.198 acc: 46.97 time: 8.02\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 52.37 %\n",
            "Average loss on the 10000 test images: 1.105\n",
            "[2,   100] loss: 1.196 acc: 47.89 time: 11.45\n",
            "[2,   200] loss: 1.110 acc: 51.62 time: 8.26\n",
            "[2,   300] loss: 1.092 acc: 52.59 time: 11.28\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 55.26 %\n",
            "Average loss on the 10000 test images: 1.049\n",
            "[3,   100] loss: 1.061 acc: 54.91 time: 9.40\n",
            "[3,   200] loss: 1.048 acc: 54.94 time: 12.59\n",
            "[3,   300] loss: 1.026 acc: 55.62 time: 8.10\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 56.70 %\n",
            "Average loss on the 10000 test images: 1.039\n",
            "[4,   100] loss: 1.008 acc: 57.77 time: 11.63\n",
            "[4,   200] loss: 0.993 acc: 57.82 time: 8.43\n",
            "[4,   300] loss: 0.987 acc: 58.27 time: 11.39\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 59.55 %\n",
            "Average loss on the 10000 test images: 0.999\n",
            "[5,   100] loss: 0.961 acc: 59.34 time: 9.53\n",
            "[5,   200] loss: 0.950 acc: 60.00 time: 11.25\n",
            "[5,   300] loss: 0.960 acc: 59.88 time: 8.15\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 62.35 %\n",
            "Average loss on the 10000 test images: 0.927\n",
            "[6,   100] loss: 0.922 acc: 61.59 time: 10.79\n",
            "[6,   200] loss: 0.924 acc: 61.19 time: 9.20\n",
            "[6,   300] loss: 0.917 acc: 62.06 time: 11.38\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 63.81 %\n",
            "Average loss on the 10000 test images: 0.883\n",
            "[7,   100] loss: 0.903 acc: 62.00 time: 9.09\n",
            "[7,   200] loss: 0.897 acc: 62.25 time: 11.53\n",
            "[7,   300] loss: 0.894 acc: 62.65 time: 8.45\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 63.72 %\n",
            "Average loss on the 10000 test images: 0.912\n",
            "[8,   100] loss: 0.882 acc: 63.14 time: 14.48\n",
            "[8,   200] loss: 0.865 acc: 64.02 time: 9.20\n",
            "[8,   300] loss: 0.866 acc: 64.05 time: 16.51\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 64.64 %\n",
            "Average loss on the 10000 test images: 0.868\n",
            "[9,   100] loss: 0.847 acc: 64.55 time: 11.82\n",
            "[9,   200] loss: 0.870 acc: 63.59 time: 9.35\n",
            "[9,   300] loss: 0.837 acc: 65.48 time: 10.58\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 66.86 %\n",
            "Average loss on the 10000 test images: 0.827\n",
            "[10,   100] loss: 0.844 acc: 65.15 time: 9.21\n",
            "[10,   200] loss: 0.835 acc: 65.65 time: 10.71\n",
            "[10,   300] loss: 0.822 acc: 66.24 time: 11.10\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 66.99 %\n",
            "Average loss on the 10000 test images: 0.809\n",
            "[11,   100] loss: 0.806 acc: 66.66 time: 11.24\n",
            "[11,   200] loss: 0.806 acc: 67.07 time: 10.38\n",
            "[11,   300] loss: 0.816 acc: 66.58 time: 9.72\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 68.09 %\n",
            "Average loss on the 10000 test images: 0.769\n",
            "[12,   100] loss: 0.793 acc: 67.66 time: 9.97\n",
            "[12,   200] loss: 0.793 acc: 68.01 time: 10.17\n",
            "[12,   300] loss: 0.789 acc: 67.95 time: 11.17\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 69.40 %\n",
            "Average loss on the 10000 test images: 0.750\n",
            "[13,   100] loss: 0.780 acc: 68.58 time: 10.22\n",
            "[13,   200] loss: 0.785 acc: 67.77 time: 10.94\n",
            "[13,   300] loss: 0.771 acc: 68.59 time: 8.91\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 68.18 %\n",
            "Average loss on the 10000 test images: 0.784\n",
            "[14,   100] loss: 0.759 acc: 69.77 time: 11.04\n",
            "[14,   200] loss: 0.754 acc: 69.52 time: 9.34\n",
            "[14,   300] loss: 0.768 acc: 68.88 time: 11.53\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 70.83 %\n",
            "Average loss on the 10000 test images: 0.722\n",
            "[15,   100] loss: 0.745 acc: 69.90 time: 9.78\n",
            "[15,   200] loss: 0.747 acc: 69.58 time: 11.54\n",
            "[15,   300] loss: 0.748 acc: 70.16 time: 8.34\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 70.55 %\n",
            "Average loss on the 10000 test images: 0.725\n",
            "[16,   100] loss: 0.706 acc: 71.54 time: 11.69\n",
            "[16,   200] loss: 0.681 acc: 72.99 time: 8.68\n",
            "[16,   300] loss: 0.668 acc: 73.59 time: 11.44\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 74.95 %\n",
            "Average loss on the 10000 test images: 0.631\n",
            "[17,   100] loss: 0.650 acc: 74.38 time: 9.22\n",
            "[17,   200] loss: 0.665 acc: 73.38 time: 11.36\n",
            "[17,   300] loss: 0.656 acc: 74.15 time: 8.52\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 74.40 %\n",
            "Average loss on the 10000 test images: 0.634\n",
            "[18,   100] loss: 0.650 acc: 74.15 time: 11.39\n",
            "[18,   200] loss: 0.660 acc: 73.70 time: 8.52\n",
            "[18,   300] loss: 0.649 acc: 74.38 time: 11.40\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.65 %\n",
            "Average loss on the 10000 test images: 0.616\n",
            "[19,   100] loss: 0.636 acc: 74.70 time: 9.50\n",
            "[19,   200] loss: 0.657 acc: 73.93 time: 11.38\n",
            "[19,   300] loss: 0.635 acc: 74.94 time: 8.35\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.97 %\n",
            "Average loss on the 10000 test images: 0.612\n",
            "[20,   100] loss: 0.633 acc: 74.87 time: 11.40\n",
            "[20,   200] loss: 0.631 acc: 75.06 time: 8.51\n",
            "[20,   300] loss: 0.627 acc: 75.39 time: 11.51\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 76.53 %\n",
            "Average loss on the 10000 test images: 0.600\n",
            "[21,   100] loss: 0.621 acc: 75.52 time: 9.36\n",
            "[21,   200] loss: 0.627 acc: 75.21 time: 11.37\n",
            "[21,   300] loss: 0.620 acc: 75.43 time: 8.18\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.57 %\n",
            "Average loss on the 10000 test images: 0.610\n",
            "[22,   100] loss: 0.627 acc: 75.12 time: 11.05\n",
            "[22,   200] loss: 0.610 acc: 76.34 time: 9.09\n",
            "[22,   300] loss: 0.622 acc: 75.36 time: 11.53\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.92 %\n",
            "Average loss on the 10000 test images: 0.604\n",
            "[23,   100] loss: 0.620 acc: 75.00 time: 9.68\n",
            "[23,   200] loss: 0.622 acc: 74.91 time: 11.28\n",
            "[23,   300] loss: 0.609 acc: 75.84 time: 8.50\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 76.42 %\n",
            "Average loss on the 10000 test images: 0.596\n",
            "[24,   100] loss: 0.598 acc: 76.21 time: 11.17\n",
            "[24,   200] loss: 0.612 acc: 75.95 time: 8.69\n",
            "[24,   300] loss: 0.608 acc: 76.35 time: 11.33\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 76.55 %\n",
            "Average loss on the 10000 test images: 0.589\n",
            "[25,   100] loss: 0.615 acc: 75.72 time: 10.07\n",
            "[25,   200] loss: 0.599 acc: 76.30 time: 10.36\n",
            "[25,   300] loss: 0.595 acc: 76.40 time: 9.08\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.04 %\n",
            "Average loss on the 10000 test images: 0.584\n",
            "[26,   100] loss: 0.595 acc: 76.47 time: 9.81\n",
            "[26,   200] loss: 0.591 acc: 76.61 time: 9.80\n",
            "[26,   300] loss: 0.601 acc: 75.88 time: 10.47\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 76.57 %\n",
            "Average loss on the 10000 test images: 0.584\n",
            "[27,   100] loss: 0.598 acc: 76.41 time: 10.94\n",
            "[27,   200] loss: 0.588 acc: 77.24 time: 9.45\n",
            "[27,   300] loss: 0.586 acc: 77.03 time: 10.06\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.12 %\n",
            "Average loss on the 10000 test images: 0.579\n",
            "[28,   100] loss: 0.590 acc: 76.73 time: 8.64\n",
            "[28,   200] loss: 0.589 acc: 76.38 time: 10.83\n",
            "[28,   300] loss: 0.601 acc: 76.00 time: 9.30\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.57 %\n",
            "Average loss on the 10000 test images: 0.574\n",
            "[29,   100] loss: 0.591 acc: 76.47 time: 11.58\n",
            "[29,   200] loss: 0.576 acc: 77.58 time: 8.90\n",
            "[29,   300] loss: 0.587 acc: 76.67 time: 10.87\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.65 %\n",
            "Average loss on the 10000 test images: 0.575\n",
            "[30,   100] loss: 0.581 acc: 77.04 time: 8.39\n",
            "[30,   200] loss: 0.576 acc: 77.64 time: 11.24\n",
            "[30,   300] loss: 0.587 acc: 76.65 time: 8.57\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.34 %\n",
            "Average loss on the 10000 test images: 0.577\n",
            "[31,   100] loss: 0.576 acc: 77.45 time: 11.64\n",
            "[31,   200] loss: 0.567 acc: 77.83 time: 8.29\n",
            "[31,   300] loss: 0.562 acc: 77.75 time: 11.26\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.09 %\n",
            "Average loss on the 10000 test images: 0.558\n",
            "[32,   100] loss: 0.573 acc: 77.41 time: 8.29\n",
            "[32,   200] loss: 0.563 acc: 78.21 time: 11.28\n",
            "[32,   300] loss: 0.565 acc: 77.97 time: 8.17\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.81 %\n",
            "Average loss on the 10000 test images: 0.561\n",
            "[33,   100] loss: 0.583 acc: 77.23 time: 11.63\n",
            "[33,   200] loss: 0.563 acc: 77.94 time: 8.14\n",
            "[33,   300] loss: 0.558 acc: 78.05 time: 11.32\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.16 %\n",
            "Average loss on the 10000 test images: 0.559\n",
            "[34,   100] loss: 0.561 acc: 78.32 time: 9.04\n",
            "[34,   200] loss: 0.575 acc: 77.43 time: 11.40\n",
            "[34,   300] loss: 0.571 acc: 77.56 time: 8.58\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.36 %\n",
            "Average loss on the 10000 test images: 0.554\n",
            "[35,   100] loss: 0.557 acc: 78.24 time: 11.24\n",
            "[35,   200] loss: 0.573 acc: 77.78 time: 8.77\n",
            "[35,   300] loss: 0.560 acc: 78.02 time: 11.22\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.09 %\n",
            "Average loss on the 10000 test images: 0.559\n",
            "[36,   100] loss: 0.565 acc: 77.95 time: 9.25\n",
            "[36,   200] loss: 0.564 acc: 77.72 time: 11.04\n",
            "[36,   300] loss: 0.564 acc: 77.94 time: 8.24\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.08 %\n",
            "Average loss on the 10000 test images: 0.554\n",
            "[37,   100] loss: 0.566 acc: 77.76 time: 10.34\n",
            "[37,   200] loss: 0.567 acc: 77.38 time: 9.20\n",
            "[37,   300] loss: 0.564 acc: 78.26 time: 10.78\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.32 %\n",
            "Average loss on the 10000 test images: 0.559\n",
            "[38,   100] loss: 0.568 acc: 78.24 time: 10.32\n",
            "[38,   200] loss: 0.551 acc: 78.73 time: 10.40\n",
            "[38,   300] loss: 0.568 acc: 77.60 time: 9.41\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.27 %\n",
            "Average loss on the 10000 test images: 0.561\n",
            "[39,   100] loss: 0.571 acc: 77.82 time: 10.28\n",
            "[39,   200] loss: 0.561 acc: 78.14 time: 9.80\n",
            "[39,   300] loss: 0.558 acc: 77.89 time: 11.05\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.20 %\n",
            "Average loss on the 10000 test images: 0.552\n",
            "[40,   100] loss: 0.568 acc: 78.04 time: 10.35\n",
            "[40,   200] loss: 0.560 acc: 77.77 time: 10.39\n",
            "[40,   300] loss: 0.567 acc: 77.57 time: 9.29\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.98 %\n",
            "Average loss on the 10000 test images: 0.557\n",
            "[41,   100] loss: 0.561 acc: 78.16 time: 10.02\n",
            "[41,   200] loss: 0.545 acc: 78.61 time: 10.08\n",
            "[41,   300] loss: 0.570 acc: 77.61 time: 10.71\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.99 %\n",
            "Average loss on the 10000 test images: 0.556\n",
            "[42,   100] loss: 0.565 acc: 77.70 time: 10.82\n",
            "[42,   200] loss: 0.566 acc: 78.05 time: 10.53\n",
            "[42,   300] loss: 0.549 acc: 78.61 time: 9.38\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.38 %\n",
            "Average loss on the 10000 test images: 0.551\n",
            "[43,   100] loss: 0.552 acc: 78.41 time: 10.21\n",
            "[43,   200] loss: 0.561 acc: 78.29 time: 9.88\n",
            "[43,   300] loss: 0.563 acc: 77.41 time: 11.18\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.31 %\n",
            "Average loss on the 10000 test images: 0.551\n",
            "[44,   100] loss: 0.550 acc: 78.59 time: 10.50\n",
            "[44,   200] loss: 0.552 acc: 78.62 time: 10.50\n",
            "[44,   300] loss: 0.559 acc: 78.35 time: 9.37\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.38 %\n",
            "Average loss on the 10000 test images: 0.551\n",
            "[45,   100] loss: 0.550 acc: 78.48 time: 10.25\n",
            "[45,   200] loss: 0.558 acc: 78.14 time: 10.00\n",
            "[45,   300] loss: 0.556 acc: 78.35 time: 10.37\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 78.18 %\n",
            "Average loss on the 10000 test images: 0.555\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "task = 'rotation'\n",
        "train(net, criterion, optimizer, num_epochs=45, decay_epochs=15, init_lr=0.01, task='rotation')\n",
        "################################\n",
        "#     TODO: Save the model     #\n",
        "################################\n",
        "torch.save(net.state_dict(), f'{task}_model.pth')\n",
        "################################\n",
        "#      End of your code        #\n",
        "################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLLMRTS9rTnk"
      },
      "source": [
        "## Fine-tuning on the pre-trained model (9 points)\n",
        "\n",
        "In this section, we will load the ResNet18 model pre-trained on the rotation task and fine-tune on the classification task. We will freeze all previous layers except for the 'layer4' block and 'fc' layer.\n",
        "\n",
        "**Then we will use the trained model from rotation task as the pretrained weights. Notice, you should not use the pretrained weights from ImageNet.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4nX4ExlrymI",
        "outputId": "73fc729d-b2f9-443e-fd0b-8eea5ab6e678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "#####################################################\n",
        "#     TODO: Load the pre-trained ResNet18 model     #\n",
        "#####################################################\n",
        "# Load the pre-trained ResNet18 model\n",
        "net = resnet18(pretrained=False,num_classes=4)\n",
        "net.load_state_dict(torch.load('rotation_model.pth',map_location='cpu'))\n",
        "# Modify the last fully connected layer to match the number of classes in your task\n",
        "fc_layers = [nn.Linear(512, 10)]\n",
        "net.fc = nn.Sequential(*fc_layers)\n",
        "print(net) # print your model and check the num_classes is correct\n",
        "####################################################\n",
        "#                End of your code                  #\n",
        "####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD44g-TxwYdU"
      },
      "outputs": [],
      "source": [
        "#################################################################################################\n",
        "#   TODO: Freeze all previous layers; only keep the 'layer4' block and 'fc' layer trainable     #\n",
        "#################################################################################################\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 'layer4' block and 'fc' layer are trainable\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in net.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "#################################################################################################\n",
        "#                                          End of your code                                     #\n",
        "#################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T5DX0efr4fh",
        "outputId": "0e69f614-94df-4c09-de77-e0d7fdeedb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.0.weight\n",
            "\t fc.0.bias\n"
          ]
        }
      ],
      "source": [
        "# Print all the trainable parameters\n",
        "params_to_update = net.parameters()\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name,param in net.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb032dG700ph"
      },
      "outputs": [],
      "source": [
        "# TODO: Define criterion and optimizer\n",
        "# Note that your optimizer only needs to update the parameters that are trainable.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vLSwOo6sBjl",
        "outputId": "ff9362c0-472c-4b51-cabe-05de24bd5a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 1.974 acc: 26.03 time: 8.74\n",
            "[1,   200] loss: 1.689 acc: 37.25 time: 6.97\n",
            "[1,   300] loss: 1.539 acc: 42.69 time: 8.70\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 48.48 %\n",
            "Average loss on the 10000 test images: 1.387\n",
            "[2,   100] loss: 1.412 acc: 48.00 time: 7.94\n",
            "[2,   200] loss: 1.388 acc: 49.23 time: 8.39\n",
            "[2,   300] loss: 1.366 acc: 49.53 time: 7.13\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 52.26 %\n",
            "Average loss on the 10000 test images: 1.306\n",
            "[3,   100] loss: 1.336 acc: 51.31 time: 8.10\n",
            "[3,   200] loss: 1.327 acc: 51.58 time: 7.77\n",
            "[3,   300] loss: 1.319 acc: 52.03 time: 8.49\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 54.57 %\n",
            "Average loss on the 10000 test images: 1.247\n",
            "[4,   100] loss: 1.290 acc: 52.58 time: 8.43\n",
            "[4,   200] loss: 1.293 acc: 52.88 time: 8.27\n",
            "[4,   300] loss: 1.279 acc: 53.48 time: 7.39\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 55.22 %\n",
            "Average loss on the 10000 test images: 1.233\n",
            "[5,   100] loss: 1.254 acc: 54.37 time: 7.42\n",
            "[5,   200] loss: 1.261 acc: 53.94 time: 8.19\n",
            "[5,   300] loss: 1.250 acc: 54.37 time: 7.85\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 56.90 %\n",
            "Average loss on the 10000 test images: 1.204\n",
            "[6,   100] loss: 1.212 acc: 56.12 time: 8.74\n",
            "[6,   200] loss: 1.254 acc: 54.38 time: 7.04\n",
            "[6,   300] loss: 1.223 acc: 55.33 time: 8.31\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 57.33 %\n",
            "Average loss on the 10000 test images: 1.180\n",
            "[7,   100] loss: 1.233 acc: 55.04 time: 7.03\n",
            "[7,   200] loss: 1.207 acc: 56.12 time: 8.65\n",
            "[7,   300] loss: 1.211 acc: 55.84 time: 6.81\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 57.17 %\n",
            "Average loss on the 10000 test images: 1.185\n",
            "[8,   100] loss: 1.193 acc: 56.67 time: 8.77\n",
            "[8,   200] loss: 1.210 acc: 56.63 time: 8.02\n",
            "[8,   300] loss: 1.196 acc: 56.61 time: 9.12\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 57.44 %\n",
            "Average loss on the 10000 test images: 1.183\n",
            "[9,   100] loss: 1.187 acc: 56.93 time: 7.20\n",
            "[9,   200] loss: 1.182 acc: 56.75 time: 8.81\n",
            "[9,   300] loss: 1.187 acc: 56.98 time: 6.84\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 57.77 %\n",
            "Average loss on the 10000 test images: 1.156\n",
            "[10,   100] loss: 1.162 acc: 57.92 time: 9.10\n",
            "[10,   200] loss: 1.176 acc: 57.46 time: 6.96\n",
            "[10,   300] loss: 1.182 acc: 56.98 time: 8.40\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 58.91 %\n",
            "Average loss on the 10000 test images: 1.143\n",
            "[11,   100] loss: 1.155 acc: 58.11 time: 7.29\n",
            "[11,   200] loss: 1.137 acc: 58.59 time: 8.71\n",
            "[11,   300] loss: 1.136 acc: 58.98 time: 6.74\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 59.84 %\n",
            "Average loss on the 10000 test images: 1.109\n",
            "[12,   100] loss: 1.125 acc: 59.43 time: 8.31\n",
            "[12,   200] loss: 1.142 acc: 58.62 time: 6.99\n",
            "[12,   300] loss: 1.130 acc: 59.00 time: 8.31\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 59.99 %\n",
            "Average loss on the 10000 test images: 1.111\n",
            "[13,   100] loss: 1.127 acc: 59.14 time: 8.31\n",
            "[13,   200] loss: 1.117 acc: 59.33 time: 9.23\n",
            "[13,   300] loss: 1.122 acc: 59.06 time: 6.90\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.27 %\n",
            "Average loss on the 10000 test images: 1.106\n",
            "[14,   100] loss: 1.126 acc: 59.38 time: 8.17\n",
            "[14,   200] loss: 1.121 acc: 59.60 time: 6.90\n",
            "[14,   300] loss: 1.114 acc: 59.66 time: 8.46\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.23 %\n",
            "Average loss on the 10000 test images: 1.103\n",
            "[15,   100] loss: 1.123 acc: 59.45 time: 8.23\n",
            "[15,   200] loss: 1.125 acc: 59.17 time: 8.01\n",
            "[15,   300] loss: 1.116 acc: 59.99 time: 7.24\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.28 %\n",
            "Average loss on the 10000 test images: 1.107\n",
            "[16,   100] loss: 1.122 acc: 59.23 time: 7.69\n",
            "[16,   200] loss: 1.113 acc: 59.90 time: 7.75\n",
            "[16,   300] loss: 1.111 acc: 59.84 time: 8.13\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.32 %\n",
            "Average loss on the 10000 test images: 1.107\n",
            "[17,   100] loss: 1.106 acc: 59.98 time: 8.64\n",
            "[17,   200] loss: 1.109 acc: 59.77 time: 6.98\n",
            "[17,   300] loss: 1.123 acc: 59.59 time: 8.09\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.54 %\n",
            "Average loss on the 10000 test images: 1.098\n",
            "[18,   100] loss: 1.089 acc: 60.87 time: 6.98\n",
            "[18,   200] loss: 1.117 acc: 59.06 time: 8.21\n",
            "[18,   300] loss: 1.111 acc: 59.45 time: 6.65\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.62 %\n",
            "Average loss on the 10000 test images: 1.095\n",
            "[19,   100] loss: 1.112 acc: 59.93 time: 10.37\n",
            "[19,   200] loss: 1.114 acc: 59.75 time: 6.51\n",
            "[19,   300] loss: 1.108 acc: 59.81 time: 8.40\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.65 %\n",
            "Average loss on the 10000 test images: 1.093\n",
            "[20,   100] loss: 1.098 acc: 60.01 time: 7.66\n",
            "[20,   200] loss: 1.107 acc: 60.12 time: 8.27\n",
            "[20,   300] loss: 1.096 acc: 60.34 time: 7.15\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 60.46 %\n",
            "Average loss on the 10000 test images: 1.098\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.001, task='classification')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghPNhcJBrcNj"
      },
      "source": [
        "## Fine-tuning on the randomly initialized model (9 points)\n",
        "In this section, we will randomly initialize a ResNet18 model and fine-tune on the classification task. We will freeze all previous layers except for the 'layer4' block and 'fc' layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RfXAh9vxXRB",
        "outputId": "de1f6ab0-8ac3-48bf-cd00-29469b90d6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "#################################################\n",
        "# TODO: Randomly initialize a ResNet18 model    #\n",
        "#################################################\n",
        "# Define the number of classes for CIFAR-10\n",
        "num_classes = 10\n",
        "\n",
        "# Create a ResNet18 model with random initialization\n",
        "net = resnet18(pretrained=False)  # Set pretrained to False for random initialization\n",
        "\n",
        "# Modify the last fully connected layer to match the number of classes in CIFAR-10\n",
        "net.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "# # Freeze all layers except 'layer4' and 'fc'\n",
        "# for name, param in net.named_parameters():\n",
        "#     if 'layer4' not in name and 'fc' not in name:\n",
        "#         param.requires_grad = False\n",
        "print(net) # print your model and check the num_classes is correct\n",
        "#################################################\n",
        "#              End of your code                 #\n",
        "#################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpx-SYAizt4p"
      },
      "outputs": [],
      "source": [
        "#################################################################################################\n",
        "# TODO: Freeze all previous layers; only keep the 'layer4' block and 'fc' layer trainable       #\n",
        "# To do this, you should set requires_grad=False for the frozen layers.                         #\n",
        "#################################################################################################\n",
        "# Freeze all previous layers except 'layer4' block and 'fc' layer\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set 'layer4' block and 'fc' layer to be trainable\n",
        "for param in net.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in net.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "#################################################################################################\n",
        "#                                          End of your code                                     #\n",
        "#################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUFWizbHxgm2",
        "outputId": "f897d423-704f-4636-afcb-cc46c114966c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t layer4.0.conv1.weight\n",
            "\t layer4.0.bn1.weight\n",
            "\t layer4.0.bn1.bias\n",
            "\t layer4.0.conv2.weight\n",
            "\t layer4.0.bn2.weight\n",
            "\t layer4.0.bn2.bias\n",
            "\t layer4.0.downsample.0.weight\n",
            "\t layer4.0.downsample.1.weight\n",
            "\t layer4.0.downsample.1.bias\n",
            "\t layer4.1.conv1.weight\n",
            "\t layer4.1.bn1.weight\n",
            "\t layer4.1.bn1.bias\n",
            "\t layer4.1.conv2.weight\n",
            "\t layer4.1.bn2.weight\n",
            "\t layer4.1.bn2.bias\n",
            "\t fc.weight\n",
            "\t fc.bias\n"
          ]
        }
      ],
      "source": [
        "# Print all the trainable parameters\n",
        "params_to_update = net.parameters()\n",
        "print(\"Params to learn:\")\n",
        "params_to_update = []\n",
        "for name,param in net.named_parameters():\n",
        "    if param.requires_grad == True:\n",
        "        params_to_update.append(param)\n",
        "        print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxFrGj091AN_"
      },
      "outputs": [],
      "source": [
        "# TODO: Define criterion and optimizer\n",
        "# Note that your optimizer only needs to update the parameters that are trainable.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzRVy0MZxpoL",
        "outputId": "498c6a60-9616-4f79-b8a4-ac4b50036711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 2.107 acc: 24.67 time: 7.93\n",
            "[1,   200] loss: 1.974 acc: 29.62 time: 8.23\n",
            "[1,   300] loss: 1.923 acc: 30.60 time: 8.10\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 35.89 %\n",
            "Average loss on the 10000 test images: 1.777\n",
            "[2,   100] loss: 1.859 acc: 33.79 time: 9.28\n",
            "[2,   200] loss: 1.829 acc: 34.50 time: 7.34\n",
            "[2,   300] loss: 1.852 acc: 33.40 time: 8.72\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 36.69 %\n",
            "Average loss on the 10000 test images: 1.755\n",
            "[3,   100] loss: 1.821 acc: 34.81 time: 7.38\n",
            "[3,   200] loss: 1.795 acc: 35.17 time: 8.87\n",
            "[3,   300] loss: 1.774 acc: 35.69 time: 7.49\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 39.15 %\n",
            "Average loss on the 10000 test images: 1.676\n",
            "[4,   100] loss: 1.759 acc: 37.16 time: 9.23\n",
            "[4,   200] loss: 1.760 acc: 37.06 time: 6.93\n",
            "[4,   300] loss: 1.766 acc: 36.47 time: 8.79\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 40.02 %\n",
            "Average loss on the 10000 test images: 1.693\n",
            "[5,   100] loss: 1.727 acc: 37.84 time: 7.27\n",
            "[5,   200] loss: 1.751 acc: 36.35 time: 8.84\n",
            "[5,   300] loss: 1.733 acc: 37.55 time: 6.89\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 41.19 %\n",
            "Average loss on the 10000 test images: 1.635\n",
            "[6,   100] loss: 1.727 acc: 37.86 time: 9.11\n",
            "[6,   200] loss: 1.709 acc: 38.45 time: 6.88\n",
            "[6,   300] loss: 1.721 acc: 38.07 time: 8.72\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 42.03 %\n",
            "Average loss on the 10000 test images: 1.625\n",
            "[7,   100] loss: 1.698 acc: 39.14 time: 7.49\n",
            "[7,   200] loss: 1.704 acc: 38.67 time: 8.50\n",
            "[7,   300] loss: 1.700 acc: 38.73 time: 6.85\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 40.72 %\n",
            "Average loss on the 10000 test images: 1.653\n",
            "[8,   100] loss: 1.677 acc: 39.78 time: 9.04\n",
            "[8,   200] loss: 1.690 acc: 39.70 time: 7.17\n",
            "[8,   300] loss: 1.693 acc: 39.27 time: 8.74\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 42.23 %\n",
            "Average loss on the 10000 test images: 1.623\n",
            "[9,   100] loss: 1.668 acc: 40.77 time: 7.85\n",
            "[9,   200] loss: 1.687 acc: 39.05 time: 8.43\n",
            "[9,   300] loss: 1.674 acc: 39.52 time: 7.14\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 42.56 %\n",
            "Average loss on the 10000 test images: 1.596\n",
            "[10,   100] loss: 1.664 acc: 39.64 time: 8.32\n",
            "[10,   200] loss: 1.657 acc: 40.25 time: 9.11\n",
            "[10,   300] loss: 1.657 acc: 40.18 time: 9.16\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 44.25 %\n",
            "Average loss on the 10000 test images: 1.596\n",
            "[11,   100] loss: 1.604 acc: 42.36 time: 7.92\n",
            "[11,   200] loss: 1.593 acc: 42.66 time: 8.78\n",
            "[11,   300] loss: 1.596 acc: 42.95 time: 6.84\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 44.86 %\n",
            "Average loss on the 10000 test images: 1.544\n",
            "[12,   100] loss: 1.593 acc: 43.08 time: 8.46\n",
            "[12,   200] loss: 1.580 acc: 43.12 time: 7.65\n",
            "[12,   300] loss: 1.594 acc: 43.09 time: 8.51\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.95 %\n",
            "Average loss on the 10000 test images: 1.528\n",
            "[13,   100] loss: 1.582 acc: 42.80 time: 8.89\n",
            "[13,   200] loss: 1.581 acc: 43.34 time: 7.82\n",
            "[13,   300] loss: 1.592 acc: 43.20 time: 7.67\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.74 %\n",
            "Average loss on the 10000 test images: 1.526\n",
            "[14,   100] loss: 1.579 acc: 43.50 time: 7.36\n",
            "[14,   200] loss: 1.580 acc: 43.32 time: 8.47\n",
            "[14,   300] loss: 1.582 acc: 43.23 time: 7.89\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 46.19 %\n",
            "Average loss on the 10000 test images: 1.522\n",
            "[15,   100] loss: 1.584 acc: 43.57 time: 9.09\n",
            "[15,   200] loss: 1.575 acc: 43.52 time: 6.83\n",
            "[15,   300] loss: 1.565 acc: 43.45 time: 8.73\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.64 %\n",
            "Average loss on the 10000 test images: 1.523\n",
            "[16,   100] loss: 1.564 acc: 43.96 time: 7.29\n",
            "[16,   200] loss: 1.564 acc: 43.38 time: 8.72\n",
            "[16,   300] loss: 1.586 acc: 43.45 time: 6.91\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.83 %\n",
            "Average loss on the 10000 test images: 1.515\n",
            "[17,   100] loss: 1.568 acc: 44.05 time: 8.94\n",
            "[17,   200] loss: 1.559 acc: 44.02 time: 6.90\n",
            "[17,   300] loss: 1.561 acc: 44.14 time: 8.50\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.81 %\n",
            "Average loss on the 10000 test images: 1.520\n",
            "[18,   100] loss: 1.575 acc: 43.73 time: 7.41\n",
            "[18,   200] loss: 1.547 acc: 44.09 time: 8.71\n",
            "[18,   300] loss: 1.564 acc: 43.91 time: 6.87\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 46.21 %\n",
            "Average loss on the 10000 test images: 1.519\n",
            "[19,   100] loss: 1.559 acc: 43.89 time: 8.93\n",
            "[19,   200] loss: 1.568 acc: 43.37 time: 6.85\n",
            "[19,   300] loss: 1.566 acc: 43.85 time: 8.82\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 46.54 %\n",
            "Average loss on the 10000 test images: 1.511\n",
            "[20,   100] loss: 1.574 acc: 43.80 time: 8.08\n",
            "[20,   200] loss: 1.551 acc: 44.69 time: 8.35\n",
            "[20,   300] loss: 1.557 acc: 44.16 time: 7.19\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 46.65 %\n",
            "Average loss on the 10000 test images: 1.511\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcN54tcNN15U"
      },
      "source": [
        "## Supervised training on the pre-trained model (9 points)\n",
        "In this section, we will load the ResNet18 model pre-trained on the rotation task and re-train the whole model on the classification task.\n",
        "\n",
        "**Then we will use the trained model from rotation task as the pretrained weights. Notice, you should not use the pretrained weights from ImageNet.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xR9h_S1N6Xi",
        "outputId": "bbd116cc-3276-4613-aad1-ec54b28d8614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "#####################################################\n",
        "#     TODO: Load the pre-trained ResNet18 model     #\n",
        "#####################################################\n",
        "# Load the pre-trained ResNet18 model\n",
        "pretrained_rotation_model = resnet18(pretrained=False,num_classes=4)\n",
        "pretrained_rotation_model.load_state_dict(torch.load('rotation_model.pth',map_location='cpu'))\n",
        "# Modify the last fully connected layer to match the number of classes in your task\n",
        "pretrained_rotation_model.fc = nn.Linear(pretrained_rotation_model.fc.in_features, 10)\n",
        "print(pretrained_rotation_model) # print your model and check the num_classes is correct\n",
        "#####################################################\n",
        "#                End of your code                   #\n",
        "#####################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGozc2cM0ADw"
      },
      "outputs": [],
      "source": [
        "# TODO: Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGWW7gzCz_Bu",
        "outputId": "6be4926b-8ddd-4f8f-bb3e-98fdc224f668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 2.260 acc: 24.50 time: 9.11\n",
            "[1,   200] loss: 1.810 acc: 33.67 time: 7.73\n",
            "[1,   300] loss: 1.876 acc: 33.41 time: 9.25\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 44.58 %\n",
            "Average loss on the 10000 test images: 1.511\n",
            "[2,   100] loss: 1.498 acc: 45.35 time: 7.96\n",
            "[2,   200] loss: 1.404 acc: 49.71 time: 9.29\n",
            "[2,   300] loss: 1.341 acc: 51.41 time: 10.37\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 56.00 %\n",
            "Average loss on the 10000 test images: 1.251\n",
            "[3,   100] loss: 1.207 acc: 56.60 time: 9.59\n",
            "[3,   200] loss: 1.147 acc: 58.91 time: 7.95\n",
            "[3,   300] loss: 1.100 acc: 60.77 time: 8.74\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 62.11 %\n",
            "Average loss on the 10000 test images: 1.096\n",
            "[4,   100] loss: 1.067 acc: 62.01 time: 8.06\n",
            "[4,   200] loss: 0.989 acc: 64.83 time: 8.89\n",
            "[4,   300] loss: 0.963 acc: 66.28 time: 9.19\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 67.83 %\n",
            "Average loss on the 10000 test images: 0.922\n",
            "[5,   100] loss: 0.932 acc: 67.16 time: 7.83\n",
            "[5,   200] loss: 0.920 acc: 67.28 time: 9.28\n",
            "[5,   300] loss: 0.909 acc: 68.20 time: 8.58\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 70.31 %\n",
            "Average loss on the 10000 test images: 0.853\n",
            "[6,   100] loss: 0.859 acc: 70.57 time: 9.60\n",
            "[6,   200] loss: 0.838 acc: 70.90 time: 8.23\n",
            "[6,   300] loss: 0.832 acc: 70.95 time: 8.44\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 70.63 %\n",
            "Average loss on the 10000 test images: 0.851\n",
            "[7,   100] loss: 0.816 acc: 71.54 time: 8.54\n",
            "[7,   200] loss: 0.774 acc: 73.09 time: 8.31\n",
            "[7,   300] loss: 0.779 acc: 72.91 time: 9.42\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 73.31 %\n",
            "Average loss on the 10000 test images: 0.769\n",
            "[8,   100] loss: 0.742 acc: 74.22 time: 8.73\n",
            "[8,   200] loss: 0.724 acc: 74.70 time: 9.35\n",
            "[8,   300] loss: 0.729 acc: 74.77 time: 7.41\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 74.86 %\n",
            "Average loss on the 10000 test images: 0.736\n",
            "[9,   100] loss: 0.711 acc: 75.36 time: 10.69\n",
            "[9,   200] loss: 0.696 acc: 75.69 time: 7.73\n",
            "[9,   300] loss: 0.693 acc: 76.19 time: 15.36\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.33 %\n",
            "Average loss on the 10000 test images: 0.734\n",
            "[10,   100] loss: 0.658 acc: 77.36 time: 9.94\n",
            "[10,   200] loss: 0.687 acc: 76.41 time: 13.37\n",
            "[10,   300] loss: 0.658 acc: 76.74 time: 9.91\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 77.55 %\n",
            "Average loss on the 10000 test images: 0.661\n",
            "[11,   100] loss: 0.577 acc: 80.10 time: 9.54\n",
            "[11,   200] loss: 0.536 acc: 81.62 time: 9.34\n",
            "[11,   300] loss: 0.532 acc: 81.45 time: 7.75\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.23 %\n",
            "Average loss on the 10000 test images: 0.545\n",
            "[12,   100] loss: 0.515 acc: 82.17 time: 8.52\n",
            "[12,   200] loss: 0.512 acc: 82.38 time: 8.41\n",
            "[12,   300] loss: 0.495 acc: 82.62 time: 9.49\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.35 %\n",
            "Average loss on the 10000 test images: 0.537\n",
            "[13,   100] loss: 0.491 acc: 83.29 time: 9.02\n",
            "[13,   200] loss: 0.482 acc: 83.52 time: 9.31\n",
            "[13,   300] loss: 0.494 acc: 82.87 time: 7.48\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.97 %\n",
            "Average loss on the 10000 test images: 0.528\n",
            "[14,   100] loss: 0.485 acc: 83.12 time: 9.49\n",
            "[14,   200] loss: 0.481 acc: 83.40 time: 7.50\n",
            "[14,   300] loss: 0.479 acc: 83.45 time: 9.26\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.96 %\n",
            "Average loss on the 10000 test images: 0.526\n",
            "[15,   100] loss: 0.475 acc: 83.52 time: 7.59\n",
            "[15,   200] loss: 0.472 acc: 83.77 time: 9.27\n",
            "[15,   300] loss: 0.466 acc: 83.73 time: 9.77\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.09 %\n",
            "Average loss on the 10000 test images: 0.522\n",
            "[16,   100] loss: 0.458 acc: 84.02 time: 9.29\n",
            "[16,   200] loss: 0.457 acc: 84.09 time: 9.10\n",
            "[16,   300] loss: 0.471 acc: 83.40 time: 7.61\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.30 %\n",
            "Average loss on the 10000 test images: 0.513\n",
            "[17,   100] loss: 0.442 acc: 84.61 time: 8.95\n",
            "[17,   200] loss: 0.447 acc: 84.16 time: 7.86\n",
            "[17,   300] loss: 0.450 acc: 83.80 time: 9.12\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.46 %\n",
            "Average loss on the 10000 test images: 0.512\n",
            "[18,   100] loss: 0.437 acc: 84.93 time: 8.40\n",
            "[18,   200] loss: 0.442 acc: 84.37 time: 9.27\n",
            "[18,   300] loss: 0.438 acc: 84.33 time: 7.40\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.38 %\n",
            "Average loss on the 10000 test images: 0.512\n",
            "[19,   100] loss: 0.433 acc: 84.84 time: 9.38\n",
            "[19,   200] loss: 0.437 acc: 84.73 time: 7.37\n",
            "[19,   300] loss: 0.424 acc: 85.28 time: 9.27\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 83.03 %\n",
            "Average loss on the 10000 test images: 0.506\n",
            "[20,   100] loss: 0.425 acc: 85.23 time: 7.50\n",
            "[20,   200] loss: 0.422 acc: 85.40 time: 9.22\n",
            "[20,   300] loss: 0.422 acc: 85.28 time: 7.36\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.80 %\n",
            "Average loss on the 10000 test images: 0.510\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVTp9jhefTi"
      },
      "source": [
        "## Supervised training on the randomly initialized model (9 points)\n",
        "In this section, we will randomly initialize a ResNet18 model and re-train the whole model on the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEjy8TBieeLK",
        "outputId": "32854e9b-23b9-49b9-fb6d-769753296de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "#################################################\n",
        "# TODO: Randomly initialize a ResNet18 model    #\n",
        "#################################################\n",
        "net = resnet18(pretrained=False)\n",
        "net.fc = nn.Linear(net.fc.in_features, num_classes)\n",
        "print(net)\n",
        "#################################################\n",
        "#              End of your code                 #\n",
        "#################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEY90pK_0ZAm"
      },
      "outputs": [],
      "source": [
        "# TODO: Define criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMDwelhY0auO",
        "outputId": "20b09919-d11a-4e85-a62f-e0ace516d388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 2.234 acc: 21.80 time: 9.30\n",
            "[1,   200] loss: 1.820 acc: 33.12 time: 7.26\n",
            "[1,   300] loss: 1.693 acc: 38.42 time: 9.14\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 45.58 %\n",
            "Average loss on the 10000 test images: 1.681\n",
            "[2,   100] loss: 1.525 acc: 44.82 time: 8.07\n",
            "[2,   200] loss: 1.461 acc: 46.68 time: 8.47\n",
            "[2,   300] loss: 1.365 acc: 50.34 time: 8.85\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 50.22 %\n",
            "Average loss on the 10000 test images: 2.207\n",
            "[3,   100] loss: 1.264 acc: 53.78 time: 8.75\n",
            "[3,   200] loss: 1.192 acc: 57.36 time: 8.85\n",
            "[3,   300] loss: 1.171 acc: 58.39 time: 7.35\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 59.98 %\n",
            "Average loss on the 10000 test images: 1.167\n",
            "[4,   100] loss: 1.069 acc: 61.64 time: 9.11\n",
            "[4,   200] loss: 1.050 acc: 62.49 time: 7.38\n",
            "[4,   300] loss: 1.032 acc: 62.76 time: 9.16\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 64.20 %\n",
            "Average loss on the 10000 test images: 1.077\n",
            "[5,   100] loss: 0.969 acc: 66.44 time: 7.90\n",
            "[5,   200] loss: 0.958 acc: 66.71 time: 10.72\n",
            "[5,   300] loss: 0.933 acc: 67.17 time: 7.29\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 66.12 %\n",
            "Average loss on the 10000 test images: 1.006\n",
            "[6,   100] loss: 0.903 acc: 68.28 time: 9.37\n",
            "[6,   200] loss: 0.883 acc: 68.91 time: 7.41\n",
            "[6,   300] loss: 0.856 acc: 69.97 time: 9.09\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 69.92 %\n",
            "Average loss on the 10000 test images: 0.852\n",
            "[7,   100] loss: 0.820 acc: 71.75 time: 7.32\n",
            "[7,   200] loss: 0.833 acc: 70.82 time: 8.76\n",
            "[7,   300] loss: 0.814 acc: 71.39 time: 7.31\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 70.38 %\n",
            "Average loss on the 10000 test images: 0.855\n",
            "[8,   100] loss: 0.762 acc: 73.70 time: 9.02\n",
            "[8,   200] loss: 0.764 acc: 73.30 time: 7.04\n",
            "[8,   300] loss: 0.785 acc: 72.84 time: 8.82\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 72.93 %\n",
            "Average loss on the 10000 test images: 0.794\n",
            "[9,   100] loss: 0.741 acc: 74.61 time: 7.33\n",
            "[9,   200] loss: 0.720 acc: 75.38 time: 8.72\n",
            "[9,   300] loss: 0.727 acc: 74.75 time: 7.50\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 75.02 %\n",
            "Average loss on the 10000 test images: 0.719\n",
            "[10,   100] loss: 0.690 acc: 75.77 time: 9.00\n",
            "[10,   200] loss: 0.707 acc: 75.67 time: 7.29\n",
            "[10,   300] loss: 0.692 acc: 76.02 time: 8.67\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 74.30 %\n",
            "Average loss on the 10000 test images: 0.747\n",
            "[11,   100] loss: 0.587 acc: 79.96 time: 7.37\n",
            "[11,   200] loss: 0.547 acc: 80.88 time: 8.88\n",
            "[11,   300] loss: 0.568 acc: 80.55 time: 8.11\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 80.30 %\n",
            "Average loss on the 10000 test images: 0.565\n",
            "[12,   100] loss: 0.516 acc: 82.02 time: 8.88\n",
            "[12,   200] loss: 0.545 acc: 80.98 time: 8.22\n",
            "[12,   300] loss: 0.528 acc: 81.59 time: 7.75\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 80.69 %\n",
            "Average loss on the 10000 test images: 0.557\n",
            "[13,   100] loss: 0.507 acc: 82.32 time: 8.46\n",
            "[13,   200] loss: 0.511 acc: 82.30 time: 7.88\n",
            "[13,   300] loss: 0.500 acc: 82.30 time: 8.93\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 80.83 %\n",
            "Average loss on the 10000 test images: 0.571\n",
            "[14,   100] loss: 0.498 acc: 82.94 time: 8.43\n",
            "[14,   200] loss: 0.499 acc: 82.69 time: 8.71\n",
            "[14,   300] loss: 0.502 acc: 82.67 time: 7.23\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.18 %\n",
            "Average loss on the 10000 test images: 0.540\n",
            "[15,   100] loss: 0.473 acc: 83.59 time: 8.68\n",
            "[15,   200] loss: 0.472 acc: 83.46 time: 8.44\n",
            "[15,   300] loss: 0.479 acc: 83.17 time: 9.59\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.24 %\n",
            "Average loss on the 10000 test images: 0.540\n",
            "[16,   100] loss: 0.470 acc: 83.42 time: 7.29\n",
            "[16,   200] loss: 0.471 acc: 83.55 time: 8.97\n",
            "[16,   300] loss: 0.476 acc: 83.52 time: 7.03\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.33 %\n",
            "Average loss on the 10000 test images: 0.546\n",
            "[17,   100] loss: 0.457 acc: 84.19 time: 8.95\n",
            "[17,   200] loss: 0.459 acc: 84.13 time: 7.09\n",
            "[17,   300] loss: 0.461 acc: 84.01 time: 8.82\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.29 %\n",
            "Average loss on the 10000 test images: 0.538\n",
            "[18,   100] loss: 0.456 acc: 84.00 time: 7.28\n",
            "[18,   200] loss: 0.452 acc: 84.12 time: 8.75\n",
            "[18,   300] loss: 0.443 acc: 84.68 time: 7.32\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 81.54 %\n",
            "Average loss on the 10000 test images: 0.533\n",
            "[19,   100] loss: 0.442 acc: 84.55 time: 8.86\n",
            "[19,   200] loss: 0.444 acc: 84.66 time: 7.51\n",
            "[19,   300] loss: 0.442 acc: 84.61 time: 8.79\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.05 %\n",
            "Average loss on the 10000 test images: 0.526\n",
            "[20,   100] loss: 0.429 acc: 84.95 time: 7.31\n",
            "[20,   200] loss: 0.426 acc: 85.01 time: 9.03\n",
            "[20,   300] loss: 0.437 acc: 84.91 time: 8.28\n",
            "TESTING:\n",
            "Accuracy of the network on the 10000 test images: 82.10 %\n",
            "Average loss on the 10000 test images: 0.528\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train(net, criterion, optimizer, num_epochs=20, decay_epochs=10, init_lr=0.01, task='classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQeYmsErkOll"
      },
      "source": [
        "# Write report (37 points)\n",
        "\n",
        "本次作業主要有3個tasks需要大家完成，在A4.pdf中有希望大家達成的baseline **(不能低於baseline最多2%，沒有達到不會給全部分數)**，report的撰寫請大家根據以下要求完成，就請大家將嘗試的結果寫在report裡，祝大家順利！\n",
        "\n",
        "1. (13 points) Train a ResNet18 on the Rotation task and report the test performance. Discuss why such a task helps in learning features that are generalizable to other visual tasks.\n",
        "\n",
        "2. (12 points) Initializing from the Rotation model or from random weights, fine-tune only the weights of the final block of convolutional layers and linear layer on the supervised CIFAR10 classification task. Report the test results and compare the performance of these two models. Provide your observations and insights. You can also discuss how the performance of pre-trained models affects downstream tasks, the performance of fine-tuning different numbers of layers, and so on.\n",
        "\n",
        "3. (12 points) Initializing from the Rotation model or from random weights, train the full network on the supervised CIFAR10 classification task. Report the test results and compare the performance of these two models. Provide your observations and insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB55-8ZMkOll"
      },
      "source": [
        "# Extra Credit (13 points)\n",
        "\n",
        "上面基本的code跟report最高可以拿到87分，這個加分部分並沒有要求同學們一定要做，若同學們想要獲得更高的分數可以根據以下的加分要求來獲得加分。\n",
        "\n",
        "- In Figure 5(b) from the Gidaris et al. paper, the authors show a plot of CIFAR10 classification performance vs. number of training examples per category for a supervised CIFAR10 model vs. a RotNet model with the final layers fine-tuned on CIFAR10. The plot shows that pre-training on the Rotation task can be advantageous when only a small amount of labeled data is available. Using your RotNet fine-tuning code and supervised CIFAR10 training code from the main assignment, try to create a similar plot by performing supervised fine-tuning/training on only a subset of CIFAR10.\n",
        "- Use a more advanced model than ResNet18 to try to get higher accuracy on the rotation prediction task, as well as for transfer to supervised CIFAR10 classification.\n",
        "  \n",
        "- If you have a good amount of compute at your disposal, try to train a rotation prediction model on the larger ImageNette dataset (still smaller than ImageNet, though).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvpW5VgekOll"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "aaa478f9632825e83f6a2247407c7a2930de96a6810af7910643e423346524f9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}